{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing and Importing Libraries"
      ],
      "metadata": {
        "id": "RuEvpf4Cifvp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_DefTM2LgAw",
        "outputId": "20e92dc9-d549-4f8f-9a3d-c767b3dde121"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234926 sha256=647be1a9877659c2067cec51495274eefed9657fe0a6e4c18fc9656f5b9542d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/b8/0f/f580817231cbf59f6ade9fd132ff60ada1de9f7dc85521f857\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pandas-profiling==2.*\n",
            "  Downloading pandas_profiling-2.13.0-py2.py3-none-any.whl (245 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting confuse>=1.0.0\n",
            "  Downloading confuse-2.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.1 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (3.1.2)\n",
            "Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (1.5.3)\n",
            "Collecting visions[type_image_path]==0.7.1\n",
            "  Downloading visions-0.7.1-py3-none-any.whl (102 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (1.10.1)\n",
            "Requirement already satisfied: requests>=2.24.0 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (2.27.1)\n",
            "Requirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (22.2.0)\n",
            "Collecting htmlmin>=0.1.12\n",
            "  Downloading htmlmin-0.1.12.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (3.7.1)\n",
            "Requirement already satisfied: missingno>=0.4.2 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (0.5.2)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (1.22.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (1.2.0)\n",
            "Collecting phik>=0.11.1\n",
            "  Downloading phik-0.12.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (679 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m679.8/679.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (4.65.0)\n",
            "Requirement already satisfied: seaborn>=0.10.1 in /usr/local/lib/python3.9/dist-packages (from pandas-profiling==2.*) (0.12.2)\n",
            "Collecting tangled-up-in-unicode>=0.0.6\n",
            "  Downloading tangled_up_in_unicode-0.2.0-py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multimethod==1.4\n",
            "  Downloading multimethod-1.4-py2.py3-none-any.whl (7.3 kB)\n",
            "Collecting bottleneck\n",
            "  Downloading Bottleneck-1.3.7-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (353 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m353.1/353.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.9/dist-packages (from visions[type_image_path]==0.7.1->pandas-profiling==2.*) (3.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from visions[type_image_path]==0.7.1->pandas-profiling==2.*) (8.4.0)\n",
            "Collecting imagehash\n",
            "  Downloading ImageHash-4.3.1-py2.py3-none-any.whl (296 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.5/296.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from confuse>=1.0.0->pandas-profiling==2.*) (6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2>=2.11.1->pandas-profiling==2.*) (2.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.0->pandas-profiling==2.*) (2.8.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.0->pandas-profiling==2.*) (4.39.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.0->pandas-profiling==2.*) (0.11.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.0->pandas-profiling==2.*) (23.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.0->pandas-profiling==2.*) (1.4.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.0->pandas-profiling==2.*) (1.0.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.0->pandas-profiling==2.*) (3.0.9)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.0->pandas-profiling==2.*) (5.12.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3->pandas-profiling==2.*) (2022.7.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.24.0->pandas-profiling==2.*) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.24.0->pandas-profiling==2.*) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.24.0->pandas-profiling==2.*) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.24.0->pandas-profiling==2.*) (2.0.12)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib>=3.2.0->pandas-profiling==2.*) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.0->pandas-profiling==2.*) (1.16.0)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.9/dist-packages (from imagehash->visions[type_image_path]==0.7.1->pandas-profiling==2.*) (1.4.1)\n",
            "Building wheels for collected packages: htmlmin\n",
            "  Building wheel for htmlmin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27096 sha256=96b66da6045137269dd09ef5d0463c67f63274af2f8d6a5657f1854075b806a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/05/04/c6d7d3b66539d9e659ac6dfe81e2d0fd4c1a8316cc5a403300\n",
            "Successfully built htmlmin\n",
            "Installing collected packages: htmlmin, tangled-up-in-unicode, multimethod, confuse, bottleneck, imagehash, visions, phik, pandas-profiling\n",
            "Successfully installed bottleneck-1.3.7 confuse-2.0.1 htmlmin-0.1.12 imagehash-4.3.1 multimethod-1.4 pandas-profiling-2.13.0 phik-0.12.3 tangled-up-in-unicode-0.2.0 visions-0.7.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting plotly==4.*\n",
            "  Downloading plotly-4.14.3-py2.py3-none-any.whl (13.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting retrying>=1.3.3\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from plotly==4.*) (1.16.0)\n",
            "Installing collected packages: retrying, plotly\n",
            "  Attempting uninstall: plotly\n",
            "    Found existing installation: plotly 5.13.1\n",
            "    Uninstalling plotly-5.13.1:\n",
            "      Successfully uninstalled plotly-5.13.1\n",
            "Successfully installed plotly-4.14.3 retrying-1.3.4\n",
            "2023-04-11 19:35:27.626544: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-11 19:35:29.514056: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-lg==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.5.0/en_core_web_lg-3.5.0-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.9/dist-packages (from en-core-web-lg==3.5.0) (3.5.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (67.6.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (23.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.1.2)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.5.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyldavis\n",
            "  Downloading pyLDAvis-3.4.0-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from pyldavis) (1.10.1)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.9/dist-packages (from pyldavis) (2.8.4)\n",
            "Collecting funcy\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (from pyldavis) (4.3.1)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.9/dist-packages (from pyldavis) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from pyldavis) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from pyldavis) (1.2.0)\n",
            "Requirement already satisfied: pandas>=1.3.4 in /usr/local/lib/python3.9/dist-packages (from pyldavis) (1.5.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from pyldavis) (67.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from pyldavis) (3.1.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.3.4->pyldavis) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.3.4->pyldavis) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.0->pyldavis) (3.1.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim->pyldavis) (6.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->pyldavis) (2.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas>=1.3.4->pyldavis) (1.16.0)\n",
            "Installing collected packages: funcy, pyldavis\n",
            "Successfully installed funcy-2.0 pyldavis-3.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (4.3.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim) (6.3.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.10.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting chart_studio\n",
            "  Downloading chart_studio-1.1.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: plotly in /usr/local/lib/python3.9/dist-packages (from chart_studio) (4.14.3)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.9/dist-packages (from chart_studio) (1.3.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from chart_studio) (1.16.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from chart_studio) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->chart_studio) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->chart_studio) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->chart_studio) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->chart_studio) (2022.12.7)\n",
            "Installing collected packages: chart_studio\n",
            "Successfully installed chart_studio-1.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting autopep8\n",
            "  Downloading autopep8-2.0.2-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tomli in /usr/local/lib/python3.9/dist-packages (from autopep8) (2.0.1)\n",
            "Collecting pycodestyle>=2.10.0\n",
            "  Downloading pycodestyle-2.10.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pycodestyle, autopep8\n",
            "Successfully installed autopep8-2.0.2 pycodestyle-2.10.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tweepy==4.10.1\n",
            "  Downloading tweepy-4.10.1-py3-none-any.whl (94 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from tweepy==4.10.1) (3.2.2)\n",
            "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from tweepy==4.10.1) (1.3.1)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.9/dist-packages (from tweepy==4.10.1) (2.27.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.27.0->tweepy==4.10.1) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.27.0->tweepy==4.10.1) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.27.0->tweepy==4.10.1) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.27.0->tweepy==4.10.1) (1.26.15)\n",
            "Installing collected packages: tweepy\n",
            "  Attempting uninstall: tweepy\n",
            "    Found existing installation: tweepy 4.13.0\n",
            "    Uninstalling tweepy-4.13.0:\n",
            "      Successfully uninstalled tweepy-4.13.0\n",
            "Successfully installed tweepy-4.10.1\n"
          ]
        }
      ],
      "source": [
        "# Installations\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    !pip install emoji --upgrade\n",
        "    !pip install pandas-profiling==2.*\n",
        "    !pip install plotly==4.*\n",
        "    !python -m spacy download en_core_web_lg\n",
        "    !pip install pyldavis\n",
        "    !pip install gensim\n",
        "    !pip install chart_studio\n",
        "    !pip install --upgrade autopep8\n",
        "    !pip install tweepy==4.10.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required Libraries\n",
        "\n",
        "#Base and Cleaning \n",
        "import tweepy\n",
        "import json\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import emoji\n",
        "import regex\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "#Visualizations\n",
        "import plotly.express as px\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt \n",
        "import pyLDAvis.gensim\n",
        "import chart_studio\n",
        "import chart_studio.plotly as py \n",
        "import chart_studio.tools as tls\n",
        "\n",
        "#Natural Language Processing (NLP)\n",
        "import spacy\n",
        "import gensim\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models.ldamulticore import LdaMulticore\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.parsing.preprocessing import STOPWORDS as SW\n",
        "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from pprint import pprint\n",
        "from wordcloud import STOPWORDS\n",
        "stopwords = set(STOPWORDS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hijK0djIL3-l",
        "outputId": "53bda586-3c11-44dd-cd7b-29e3941a2b64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/google/rpc/__init__.py:20: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.rpc')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  pkg_resources.declare_namespace(__name__)\n",
            "/usr/local/lib/python3.9/dist-packages/pkg_resources/__init__.py:2349: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(parent)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions\n"
      ],
      "metadata": {
        "id": "X71qtL-mitE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Removes emojis and url's from tweets\n",
        "def clean_tweet(text):\n",
        "    emoji_free_text= emoji.replace_emoji(text, '')\n",
        "    clean_text = re.sub(r'http\\S+', '', emoji_free_text)\n",
        "    return clean_text"
      ],
      "metadata": {
        "id": "QMS86EXfLvNf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "048d0ffd-fbf0-4eba-d14c-7a8092a9e0c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Performs lemmatization in a text\n",
        "def get_lemmas(text):\n",
        "    lemmas = []\n",
        "    \n",
        "    doc = nlp(text)\n",
        "    \n",
        "    for token in doc: \n",
        "        if ((token.is_stop == False) and (token.is_punct == False)) and (token.pos_ != 'PRON'):\n",
        "            lemmas.append(token.lemma_)\n",
        "    \n",
        "    return lemmas"
      ],
      "metadata": {
        "id": "3MfE63ntmwrI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "168e95dd-31a4-433e-bfe0-ed9d63feb8d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer function\n",
        "def tokenize(text):\n",
        "\n",
        "    text = re.sub('[^a-zA-Z 0-9]', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
        "    text = re.sub('@!$', '', text) # Remove @ ! $\n",
        "\n",
        "    tokens = text.lower().split() # Make text lowercase and split it\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "xwn63POsoph0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba9e5d7c-7564-415a-c7c3-3b61a883ada4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining a function to loop over number of topics to be used to find an  optimal number of topics via coherence value\n",
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=2):\n",
        "    coherence_values_topic = []\n",
        "    model_list_topic = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
        "        model_list_topic.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values_topic.append(coherencemodel.get_coherence())\n",
        "\n",
        "    return model_list_topic, coherence_values_topic    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuTQSdqw52sK",
        "outputId": "e9447419-9a64-446e-c504-9a28486ca016"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gets the contribution score for each tweet according to its most relevant topic\n",
        "def format_topics_sentences(ldamodel, corpus, tweets):\n",
        "\n",
        "    sent_topics_df = pd.DataFrame()\n",
        "\n",
        "    # Get main topic in each document\n",
        "    for i, row in enumerate(ldamodel[corpus]):\n",
        "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\n",
        "            if j == 0:  # => dominant topic\n",
        "                wp = ldamodel.show_topic(topic_num)\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
        "            else:\n",
        "                break\n",
        "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
        "\n",
        "    # Add original text to the end of the output\n",
        "    contents = pd.Series(tweets)\n",
        "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
        "    return(sent_topics_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYhhg7lXCNl4",
        "outputId": "91e76179-5af1-429c-c2b1-e2a1f131591a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Gathering"
      ],
      "metadata": {
        "id": "YZbqun_ZiwIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Twitter Keys\n",
        "\n",
        "consumer_key = '0U8DU7xm6LokJIY7NVsxof2iN'\n",
        "consumer_secret = 'laooGyUhS0qtRDySqeTXPQBZPFETxRZHAkZYCiggl4aChr0jrI'\n",
        "access_token = '1494139415911931904-nbBK8CVScmPJGwLVQASzYBjbFBXBPs'\n",
        "access_token_secret = 'KSM1PSpvYAqjzV2AsYKa4dqSGGWDtr51SYorfED5f5Hnz'\n",
        "bearer_token=\"AAAAAAAAAAAAAAAAAAAAAJ6ClgEAAAAA25Q%2FW0u3vVZf9dxt7vfyQcFMKw8%3DbFwfWjWXcBQ4tovioM4zCuxBrX1KjPGtcKi8iwlCJY9OevSmc6\""
      ],
      "metadata": {
        "id": "dtLV8d5zMhuE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "799dbac6-97f8-42e8-93e9-c00dd1de8565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializing the Twitter client\n",
        "client = tweepy.Client(\n",
        "    bearer_token=bearer_token,\n",
        "    consumer_key=consumer_key,\n",
        "    consumer_secret=consumer_secret,\n",
        "    access_token=access_token,\n",
        "    access_token_secret=access_token_secret,\n",
        "    return_type = requests,\n",
        "    wait_on_rate_limit=True\n",
        ")"
      ],
      "metadata": {
        "id": "RDtPTsw8SC8H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9b84b5b-e676-4c36-e25e-9834fdf93cf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Twitter handle of the subject of our tweet topic modeling\n",
        "user_twitter_name = 'elonmusk'\n",
        "\n",
        "# Fetch user data\n",
        "user = client.get_user(username=user_twitter_name)\n",
        "\n",
        "# Extract the user id and user name\n",
        "user_id = user.data.id\n",
        "\n",
        "# Fetch tweets by the user \n",
        "tweets = client.get_users_tweets(id = user_id, max_results=100)"
      ],
      "metadata": {
        "id": "_6X9AUSJVuLZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04ecdebc-d34e-49d8-b88f-a1256c70d6d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the Dataframe to store the tweets\n",
        "tweets_df = pd.DataFrame(columns=['original_tweet'])"
      ],
      "metadata": {
        "id": "oPLMuljuSDDR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a1553cc-70b6-43ca-fe66-149cf32b3969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleansing"
      ],
      "metadata": {
        "id": "M5Y0k2V1i5Fi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the tweets text to the dataframe\n",
        "tweets_text = []\n",
        "for tweet in tweets.data:\n",
        "  tweets_text.append(tweet.text)\n",
        "\n",
        "tweets_df['original_tweet'] = tweets_text"
      ],
      "metadata": {
        "id": "NmKNlyFZKxQj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4be2170e-65d9-48bf-c135-6a0be358252f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cleaning the text from emojis and url's and appending it to a new column\n",
        "tweets_df['clean_tweet'] = tweets_df['original_tweet'].apply(clean_tweet)"
      ],
      "metadata": {
        "id": "rnkWrRl3LW6h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fe79662-d78c-4598-82e3-c30d8ee686a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-processing"
      ],
      "metadata": {
        "id": "UnKiJvehi93i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading Spacy\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = Tokenizer(nlp.vocab)\n",
        "\n",
        "# Custom stopwords\n",
        "custom_stopwords = ['hi','\\n','\\n\\n', '&', ' ', '.', '-', 'got', \"it's\", 'it’s', \"i'm\", 'i’m', 'im', 'want', 'like', '$', '@']\n",
        "\n",
        "# Customize stop words by adding to the default list\n",
        "STOP_WORDS = nlp.Defaults.stop_words.union(custom_stopwords)\n",
        "\n",
        "# ALL_STOP_WORDS = spacy + gensim + wordcloud\n",
        "ALL_STOP_WORDS = STOP_WORDS.union(SW).union(stopwords)\n",
        "\n",
        "tokens = []\n",
        "\n",
        "for doc in tokenizer.pipe(tweets_df['clean_tweet'], batch_size=500):\n",
        "    doc_tokens = []    \n",
        "    for token in doc: \n",
        "        if token.text.lower() not in STOP_WORDS:\n",
        "            doc_tokens.append(token.text.lower())   \n",
        "    tokens.append(doc_tokens)\n",
        "\n",
        "# Makes tokens column\n",
        "tweets_df['tokens'] = tokens"
      ],
      "metadata": {
        "id": "L0Y3ESEPNgu1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34565ff2-4984-4f56-9287-5957af488177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make tokens a string again and lemmatize the string\n",
        "token_strings = [' '.join(map(str, l)) for l in tweets_df['tokens']]\n",
        "lemmas = []\n",
        "\n",
        "for text in token_strings:\n",
        "   lemmas.append(get_lemmas(text))\n",
        "\n",
        "tweets_df['lemmas'] = lemmas"
      ],
      "metadata": {
        "id": "EsUyGVzhkFTY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6f0df1b-deaa-45e3-b0dd-0f31ea462c05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make lemmatizations a string again and clean the words to get the final tokens\n",
        "lemmas_strings = [' '.join(map(str, l)) for l in tweets_df['lemmas']]\n",
        "                 \n",
        "final_tokens = []\n",
        "\n",
        "for text in lemmas_strings:\n",
        "   final_tokens.append(tokenize(text))\n",
        "\n",
        "tweets_df['final_tokens'] = final_tokens"
      ],
      "metadata": {
        "id": "cAF7y_65n8fp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c305efd6-9714-4617-f6fa-c58a67c10681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topic Modeling"
      ],
      "metadata": {
        "id": "FQSyKd2O3jS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a id2word dictionary\n",
        "id2word = Dictionary(tweets_df['final_tokens'])\n",
        "print(len(id2word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soEJQZJpqmac",
        "outputId": "f65240d6-a448-4802-8d79-66977a68992b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering Extremes\n",
        "id2word.filter_extremes(no_below=2, no_above=.99)\n",
        "print(len(id2word))"
      ],
      "metadata": {
        "id": "XB8ylrdHrGXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1721d78-32e9-462f-c507-62c807b7a5a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "103\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a corpus object \n",
        "corpus = [id2word.doc2bow(d) for d in tweets_df['final_tokens']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0pTmAPk1uxo",
        "outputId": "81c1052e-5274-47fe-f649-ccad3dbeeada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiating a Base LDA model \n",
        "base_model = LdaMulticore(corpus=corpus, num_topics=10, id2word=id2word, workers=12, passes=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jyCBO4J10st",
        "outputId": "9e2e9afe-4c87-4689-fbbe-5bb0bb38c705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering for words \n",
        "words = [re.findall(r'\"([^\"]*)\"',t[1]) for t in base_model.print_topics()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcrBEvHj2AtZ",
        "outputId": "134837ac-7ddd-4798-f7f2-642012870eb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Topics\n",
        "topics = [' '.join(t[0:10]) for t in words]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdM4-XQF2DOv",
        "outputId": "67900057-c96d-4da3-871c-2351e1bd0b44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the topics\n",
        "for id, t in enumerate(topics): \n",
        "    print(f\"------ Topic {id} ------\")\n",
        "    print(t, end=\"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXxqnig12E2O",
        "outputId": "432ae8b4-516f-4dda-e4f4-03d4f1c3d4dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------ Topic 0 ------\n",
            "twitter say timsweeneyepic old amp heavy titterdaily rt year ago\n",
            "\n",
            "------ Topic 1 ------\n",
            "actually yeah know matter company davidasinclair think great wallstreetsilv bbc\n",
            "\n",
            "------ Topic 2 ------\n",
            "influence iamharaldur bbc fund davidmweissman pay principle need lol think\n",
            "\n",
            "------ Topic 3 ------\n",
            "press think drug legacy public scottadamssays kanekoathegreat crime micsolana yes\n",
            "\n",
            "------ Topic 4 ------\n",
            "people right good actually question let criminal time sf micsolana\n",
            "\n",
            "------ Topic 5 ------\n",
            "probably wallstreetsilv year wow scottadamssays 10 lack live problem try\n",
            "\n",
            "------ Topic 6 ------\n",
            "tesla talent wrong amp heavy paulg industry yeah end rapidly\n",
            "\n",
            "------ Topic 7 ------\n",
            "amp jonerlichman day billym2k titterdaily dogeofficialceo true cbdoge buy tell\n",
            "\n",
            "------ Topic 8 ------\n",
            "medium scottadamssays starship world long shellenbergermd trungtphan oh tbh twitter\n",
            "\n",
            "------ Topic 9 ------\n",
            "therabbithole84 supplement open teslaownerssv california good trungtphan case yes tesla\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Perplexity a measure of how good the model is. lower the better\n",
        "base_perplexity = base_model.log_perplexity(corpus)\n",
        "print('\\nPerplexity: ', base_perplexity) \n",
        "\n",
        "# Compute Coherence Score\n",
        "coherence_model = CoherenceModel(model=base_model, texts=tweets_df['final_tokens'], \n",
        "                                   dictionary=id2word, coherence='c_v')\n",
        "coherence_lda_model_base = coherence_model.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda_model_base)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_bHYqw52d70",
        "outputId": "c30c390d-0319-4331-b026-b93b0115b381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Perplexity:  -5.784151459242444\n",
            "\n",
            "Coherence Score:  0.5349604213963334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Topic Distance Visualization \n",
        "pyLDAvis.enable_notebook()\n",
        "pyLDAvis.gensim.prepare(base_model, corpus, id2word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 951
        },
        "id": "p0WRHYju2wvJ",
        "outputId": "3f88bf8a-01a6-49a6-d2c3-7a7d60412ad0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.9/dist-packages/pyLDAvis/_prepare.py:243: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
            "  default_term_info = default_term_info.sort_values(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
              "topic                                                \n",
              "4     -0.142624 -0.114549       1        1  18.743223\n",
              "3     -0.079438 -0.078654       2        1  14.731631\n",
              "0      0.044744 -0.058512       3        1  11.415965\n",
              "7      0.118137 -0.061394       4        1  11.112068\n",
              "2     -0.065461  0.202701       5        1  10.444800\n",
              "6      0.194778  0.018696       6        1   9.384948\n",
              "8     -0.013949 -0.012453       7        1   8.814933\n",
              "1     -0.036590  0.085320       8        1   5.855957\n",
              "5     -0.025739  0.007988       9        1   5.247088\n",
              "9      0.006142  0.010855      10        1   4.249387, topic_info=              Term      Freq     Total Category  logprob  loglift\n",
              "27            good  5.000000  5.000000  Default  30.0000  30.0000\n",
              "95           tesla  3.000000  3.000000  Default  29.0000  29.0000\n",
              "6         actually  5.000000  5.000000  Default  28.0000  28.0000\n",
              "26  wallstreetsilv  3.000000  3.000000  Default  27.0000  27.0000\n",
              "11          people  5.000000  5.000000  Default  26.0000  26.0000\n",
              "..             ...       ...       ...      ...      ...      ...\n",
              "1            right  0.060497  5.355036  Topic10  -5.3132  -1.3248\n",
              "11          people  0.060497  5.431639  Topic10  -5.3132  -1.3390\n",
              "47           think  0.060497  5.281158  Topic10  -5.3132  -1.3109\n",
              "76        thinking  0.060497  2.258403  Topic10  -5.3132  -0.4614\n",
              "6         actually  0.060497  5.895638  Topic10  -5.3132  -1.4210\n",
              "\n",
              "[415 rows x 6 columns], token_table=      Topic      Freq     Term\n",
              "term                          \n",
              "59        4  0.355074       10\n",
              "59        6  0.355074       10\n",
              "59        9  0.355074       10\n",
              "85        2  0.440484  account\n",
              "85        3  0.440484  account\n",
              "...     ...       ...      ...\n",
              "63        3  0.277181     year\n",
              "63        4  0.277181     year\n",
              "63        9  0.277181     year\n",
              "38        2  0.474856      yes\n",
              "38       10  0.474856      yes\n",
              "\n",
              "[219 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[5, 4, 1, 8, 3, 7, 9, 2, 6, 10])"
            ],
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el684140097772805248710964983\" style=\"background-color:white;\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el684140097772805248710964983_data = {\"mdsDat\": {\"x\": [-0.14262422730492957, -0.07943789091476826, 0.044743823013824444, 0.1181371692207054, -0.06546109446410228, 0.1947781640954976, -0.013949375014738056, -0.03658979778807914, -0.025738858135296964, 0.0061420872918874604], \"y\": [-0.11454864140577128, -0.07865354811217519, -0.058511956151929716, -0.06139352549762577, 0.20270140948748222, 0.0186958672107885, -0.012452531823487304, 0.08531968854480677, 0.007988464366787153, 0.010854773381124267], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [18.743222767949693, 14.731630919848232, 11.41596464641453, 11.112068255711836, 10.444800036895026, 9.384948268546452, 8.814932857384479, 5.855957243369046, 5.247087749152159, 4.249387254728545]}, \"tinfo\": {\"Term\": [\"good\", \"tesla\", \"actually\", \"wallstreetsilv\", \"people\", \"yeah\", \"probably\", \"talent\", \"know\", \"bbc\", \"influence\", \"iamharaldur\", \"scottadamssays\", \"amp\", \"matter\", \"trungtphan\", \"jonerlichman\", \"right\", \"therabbithole84\", \"medium\", \"california\", \"heavy\", \"case\", \"yes\", \"open\", \"teslaownerssv\", \"supplement\", \"paulg\", \"10\", \"think\", \"people\", \"question\", \"right\", \"criminal\", \"fix\", \"good\", \"friend\", \"sf\", \"let\", \"time\", \"micsolana\", \"actually\", \"open\", \"teslaownerssv\", \"try\", \"take\", \"world\", \"rt\", \"propaganda\", \"alcohol\", \"crime\", \"probably\", \"think\", \"tell\", \"ago\", \"buy\", \"true\", \"shellenbergermd\", \"help\", \"wrong\", \"drug\", \"twitter\", \"kanekoathegreat\", \"press\", \"public\", \"legacy\", \"drug\", \"yes\", \"lack\", \"robbysoave\", \"investmattallen\", \"tbh\", \"long\", \"thinking\", \"need\", \"rapidly\", \"reduce\", \"platform\", \"importantly\", \"account\", \"remove\", \"check\", \"crime\", \"alcohol\", \"propaganda\", \"think\", \"know\", \"therabbithole84\", \"true\", \"buy\", \"shellenbergermd\", \"greatly\", \"kanekoathegreat\", \"scottadamssays\", \"micsolana\", \"time\", \"titterdaily\", \"let\", \"amp\", \"timsweeneyepic\", \"old\", \"say\", \"problem\", \"w\", \"post\", \"launch\", \"require\", \"check\", \"remove\", \"account\", \"importantly\", \"platform\", \"reduce\", \"rt\", \"twitter\", \"heavy\", \"ago\", \"greatly\", \"help\", \"legacy\", \"kanekoathegreat\", \"sf\", \"year\", \"titterdaily\", \"davidmweissman\", \"influence\", \"amp\", \"fix\", \"question\", \"jonerlichman\", \"billym2k\", \"dogeofficialceo\", \"day\", \"titterdaily\", \"supplement\", \"live\", \"company\", \"davidasinclair\", \"oh\", \"amp\", \"lol\", \"cbdoge\", \"w\", \"10\", \"tell\", \"paulg\", \"ago\", \"true\", \"buy\", \"yeah\", \"trungtphan\", \"year\", \"davidmweissman\", \"twitter\", \"right\", \"matter\", \"wow\", \"case\", \"california\", \"scottadamssays\", \"iamharaldur\", \"good\", \"therabbithole84\", \"iamharaldur\", \"fund\", \"pay\", \"influence\", \"principle\", \"bbc\", \"davidmweissman\", \"case\", \"great\", \"lol\", \"way\", \"silly\", \"need\", \"thinking\", \"therabbithole84\", \"medium\", \"greatly\", \"help\", \"friend\", \"think\", \"actually\", \"supplement\", \"live\", \"matter\", \"wow\", \"california\", \"company\", \"davidasinclair\", \"yes\", \"teslaownerssv\", \"dogeofficialceo\", \"trungtphan\", \"yeah\", \"good\", \"wrong\", \"titterdaily\", \"wallstreetsilv\", \"cbdoge\", \"talent\", \"industry\", \"tesla\", \"paulg\", \"heavy\", \"wrong\", \"california\", \"cbdoge\", \"end\", \"silly\", \"require\", \"rapidly\", \"10\", \"say\", \"yeah\", \"amp\", \"principle\", \"supplement\", \"live\", \"matter\", \"wow\", \"case\", \"jonerlichman\", \"company\", \"davidasinclair\", \"yes\", \"teslaownerssv\", \"open\", \"problem\", \"lack\", \"dogeofficialceo\", \"trungtphan\", \"good\", \"wallstreetsilv\", \"iamharaldur\", \"titterdaily\", \"therabbithole84\", \"scottadamssays\", \"tell\", \"right\", \"try\", \"probably\", \"starship\", \"medium\", \"scottadamssays\", \"wow\", \"oh\", \"way\", \"end\", \"launch\", \"post\", \"long\", \"tbh\", \"investmattallen\", \"robbysoave\", \"world\", \"take\", \"day\", \"shellenbergermd\", \"wallstreetsilv\", \"trungtphan\", \"twitter\", \"actually\", \"amp\", \"supplement\", \"live\", \"matter\", \"case\", \"california\", \"davidasinclair\", \"company\", \"yes\", \"dogeofficialceo\", \"iamharaldur\", \"yeah\", \"good\", \"jonerlichman\", \"therabbithole84\", \"titterdaily\", \"tell\", \"matter\", \"know\", \"yeah\", \"company\", \"davidasinclair\", \"great\", \"tell\", \"actually\", \"wallstreetsilv\", \"bbc\", \"think\", \"supplement\", \"live\", \"wow\", \"case\", \"california\", \"yes\", \"teslaownerssv\", \"open\", \"problem\", \"lack\", \"try\", \"billym2k\", \"dogeofficialceo\", \"oh\", \"starship\", \"lol\", \"cbdoge\", \"w\", \"way\", \"trungtphan\", \"iamharaldur\", \"titterdaily\", \"scottadamssays\", \"good\", \"jonerlichman\", \"wrong\", \"amp\", \"therabbithole84\", \"principle\", \"right\", \"people\", \"timsweeneyepic\", \"davidmweissman\", \"probably\", \"wallstreetsilv\", \"live\", \"wow\", \"problem\", \"lack\", \"try\", \"10\", \"bbc\", \"scottadamssays\", \"year\", \"good\", \"supplement\", \"matter\", \"case\", \"california\", \"davidasinclair\", \"company\", \"yes\", \"teslaownerssv\", \"open\", \"billym2k\", \"dogeofficialceo\", \"great\", \"oh\", \"starship\", \"lol\", \"cbdoge\", \"w\", \"way\", \"trungtphan\", \"therabbithole84\", \"principle\", \"yeah\", \"iamharaldur\", \"amp\", \"jonerlichman\", \"tell\", \"titterdaily\", \"wrong\", \"think\", \"timsweeneyepic\", \"davidmweissman\", \"world\", \"supplement\", \"case\", \"california\", \"yes\", \"open\", \"teslaownerssv\", \"therabbithole84\", \"trungtphan\", \"tesla\", \"good\", \"live\", \"matter\", \"wow\", \"davidasinclair\", \"company\", \"problem\", \"lack\", \"try\", \"billym2k\", \"dogeofficialceo\", \"great\", \"oh\", \"starship\", \"lol\", \"cbdoge\", \"w\", \"way\", \"end\", \"launch\", \"post\", \"jonerlichman\", \"yeah\", \"scottadamssays\", \"titterdaily\", \"iamharaldur\", \"principle\", \"wrong\", \"tell\", \"wallstreetsilv\", \"amp\", \"probably\", \"right\", \"people\", \"think\", \"thinking\", \"actually\"], \"Freq\": [5.0, 3.0, 5.0, 3.0, 5.0, 3.0, 2.0, 3.0, 2.0, 3.0, 3.0, 2.0, 3.0, 8.0, 2.0, 3.0, 2.0, 5.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 5.0, 4.7912190096084375, 2.3484454515690376, 3.9475449093275445, 1.649431632929931, 1.502949882966011, 3.2203434824777357, 1.649422249638846, 1.6494246711333196, 1.6494322383035493, 1.6494290096442512, 1.6494241666553042, 2.4348740376930227, 0.8639796430845481, 0.8639696544198447, 0.8639914478701067, 0.8639741947219827, 0.863951594106896, 0.8639593630683321, 0.8639853941339228, 0.8639827708482432, 0.8639767171120593, 0.8639212245303735, 1.6493854227437272, 0.8639873111503811, 0.8639647105352946, 0.8639882192108087, 0.8639723786011274, 0.8639651141177067, 0.8639795421889451, 0.8639234442336409, 0.8639793403977389, 0.8639776251724869, 0.863967132029768, 2.424180643825939, 1.6421818124874263, 1.6421818124874263, 1.6421889495797173, 0.8601894045319756, 0.8601860738889066, 0.8601821881386594, 0.8601814744294303, 0.8601742580361141, 0.8601599045505072, 0.8601718790053505, 0.8601729099186814, 0.8601813158273794, 0.8601892459299247, 0.8601861531899321, 0.8601855980827539, 0.8601827432458375, 0.8601757647555976, 0.8601750510463686, 0.8601923386699174, 0.8601883736186448, 0.8601804435160995, 1.6422016377437896, 0.8601718790053505, 0.8601715618012487, 0.8601775886791831, 0.8601713238981723, 0.8601867082971102, 0.8601871841032629, 0.8602024099001498, 0.8882902780083817, 0.8601919421647901, 0.8601892459299247, 0.8601885322206956, 0.8601848843735248, 0.8601769542709795, 1.6154988730995976, 1.6154650741441827, 1.6155083368071137, 0.8462165807811779, 0.8462176869288096, 0.8462144299385606, 0.84620779305277, 0.846212217643297, 0.8462240165513691, 0.8462221729719829, 0.8462183629079179, 0.8462144299385606, 0.846214245580622, 0.8462120947380046, 0.8462269662783871, 1.6155324262444275, 0.8738676291113033, 0.8462247539831236, 0.846208100316001, 0.846213692506806, 0.8462162120653006, 0.8462066869051382, 0.8462193461502573, 0.8462263517519251, 0.846234586406517, 0.8462185472658565, 0.8462072399789541, 0.873888768821599, 0.2203892382302031, 0.1615785180978284, 2.1755948412565527, 1.4910896623106413, 1.4655365412894608, 1.4911168191189552, 1.4910644196385963, 0.781042368627095, 0.7810481110359014, 0.7810211336778627, 0.7810207149605539, 0.7810313623435493, 2.91125770465705, 0.7810324988619589, 0.781059536036756, 0.7810419499097863, 0.781044163129847, 0.7810531954603656, 0.7810413517422022, 0.7810513411408551, 0.7810609118221992, 0.7810546310625671, 0.7809811162664927, 0.7810517000414056, 0.7810511616905799, 0.7810443425801222, 0.7810419499097863, 0.7810168866880163, 0.07100600645614648, 0.07100555035336367, 0.07100544567403648, 0.07100521388409767, 0.07100675416562649, 0.07100647751311888, 0.07100647751311888, 0.07100637283379169, 2.3208984316595886, 1.5906807808262853, 1.5906757205921382, 2.3481611740493444, 1.277345010162112, 1.590709343036804, 1.5906766201893199, 0.8332160181042578, 0.8332064598842024, 0.8332215843618196, 0.8332105642963439, 0.8332127570644742, 0.8332228775327682, 0.8332149498326046, 0.8332007249521691, 0.8332103956218723, 0.8332178172986212, 0.8332115201183494, 0.8332156245304908, 0.8332209096639333, 0.8332160181042578, 0.07574744615041425, 0.07574767807781266, 0.07574857064689136, 0.07574775538694545, 0.0757473899255904, 0.07574775538694545, 0.0757476991621216, 0.07574822626984525, 0.07574836683190489, 0.07574967405905952, 0.07574904152979116, 0.07574897124876133, 0.07574891502393748, 0.0757487533775689, 0.0757487463494659, 0.07574872526515697, 0.07574857767499435, 2.3387229912435394, 1.5754367843555506, 2.3661911010497922, 1.5754428467068997, 1.5754533547825715, 1.5754839696568848, 0.8396083812765202, 0.8395777158826123, 0.8396151003825988, 0.8396068151690883, 0.839610553619087, 0.8396134332359778, 0.8396107051778706, 0.8396071688062503, 0.8396328327602951, 1.5754733605420237, 0.3920458962807616, 0.07632898821884945, 0.07632927239156895, 0.07632975232771742, 0.07632921555702506, 0.07632909557298793, 0.10377920777367253, 0.07632928502146759, 0.0763291713523798, 0.07632949341479522, 0.07632984705195725, 0.07632882403016708, 0.07632919029722776, 0.0763291713523798, 0.07633139421454116, 0.07633057327112928, 0.07633054169638268, 0.07633031435820709, 0.07633030172830844, 0.07633027015356184, 0.07633020700406862, 0.076330175429322, 0.07633011227982878, 0.07632989757155183, 0.07632989757155183, 0.07632984705195725, 1.4925955388861394, 1.518613493323167, 1.492609679336655, 0.7954526801525024, 0.7954694304177106, 0.7954594182195266, 0.7954532021154409, 0.7954627872530388, 0.7954614586201044, 0.7954798222253042, 0.7954685288453622, 0.795465349616555, 0.7954651123606739, 0.7954804390905952, 0.7954654445189074, 0.7954352655708271, 0.7954753618147389, 0.7954309475137904, 0.7954732265118087, 0.7954667257006656, 0.7954188000126763, 0.795465349616555, 0.07231550953039292, 0.07231580610024434, 0.0723162568864185, 0.07231578830605326, 0.07231549173620183, 0.07231579423745028, 0.07231565781531864, 0.0723160908073017, 0.07231735419486875, 0.07231722963553115, 0.07231703983082624, 0.07231700424244407, 0.07231699831104704, 0.0723168025749451, 0.07231674919237185, 0.07231674326097483, 1.3826228387114616, 1.40673178851739, 1.4067483065058242, 0.7368717511566616, 0.7368712467906026, 0.7368541613903521, 0.7368302040025468, 1.40676116784033, 0.7368537831158077, 0.7368510721482403, 0.7368664553130415, 0.06698725204206901, 0.06698746482150017, 0.06698755939013624, 0.06698740965646247, 0.06698721263847064, 0.06698774064668872, 0.0669882056091494, 0.06698722051919032, 0.06698749634437887, 0.06698740177574279, 0.06698792978396086, 0.06698759091301493, 0.06698937195566097, 0.06698746482150017, 0.0669878588574838, 0.06698730720710672, 0.06698785097676413, 0.06698731508782639, 0.06698729144566737, 0.06698874937880683, 0.06698851295721664, 0.06698845779217893, 0.06698844991145926, 0.06698833170066418, 0.06698833170066418, 0.06698830805850516, 0.06698826077418712, 0.06698826077418712, 0.06698822925130843, 0.06698805587547563, 0.06698790614180185, 0.06698781945388545, 0.06698781157316577, 1.31051197427829, 1.3104880222578845, 0.6864311799247017, 0.6864386366857713, 0.6864303325654892, 0.6864338349835674, 0.686419203914499, 0.6864347953240082, 0.6864121425877284, 0.6864384672139289, 0.6864466583529826, 0.6864177916491448, 0.06240398104130125, 0.062404609499383826, 0.06240405165456896, 0.062403860998746154, 0.06240416463579729, 0.062404143451816975, 0.062404327046313005, 0.06240467305132476, 0.06240385393741938, 0.062404235249064986, 0.062405478042576595, 0.06240399516395479, 0.06240427055569884, 0.062404143451816975, 0.062404030470588644, 0.06240469423530507, 0.06240402340926188, 0.06240395985732094, 0.06240544273594274, 0.062405209712159314, 0.06240518852817901, 0.06240517440552546, 0.06240514616021838, 0.06240490607510819, 0.062404884891127874, 0.06240477190989955, 0.06240474366459246, 0.06240464480601768, 0.0624045671314232, 0.06240440472090748, 0.06240436235294686, 0.06240434823029332, 0.6654580268608852, 0.6654485110102797, 0.6654545041661899, 0.6654459947997831, 0.6654572033738135, 0.6654545956647534, 0.6654624645412156, 0.6654489685030973, 0.665445445808402, 0.6654526284456378, 0.060496820071121254, 0.06049715175341399, 0.06049690013236433, 0.060496808633800817, 0.06049671141657708, 0.06049671141657708, 0.06049666566729532, 0.060497037380209596, 0.060496734291217956, 0.06049771790077573, 0.06049658560605225, 0.060496785759159934, 0.06049690013236433, 0.06049676288451906, 0.060497391937143215, 0.060496602762032906, 0.06049663135533401, 0.06049653413811027, 0.06049675716585884, 0.060496425483566096, 0.060497912335223204, 0.06049789517924254, 0.06049780939933925, 0.06049772361943596, 0.060497689307474636, 0.060497660714173535, 0.060497437686424974, 0.060497437686424974, 0.0604973290318808, 0.060497248970637726, 0.06049717462805487, 0.060497083129491355, 0.0604970659735107, 0.06049705453619026, 0.060497043098869815, 0.06049698591226762], \"Total\": [5.0, 3.0, 5.0, 3.0, 5.0, 3.0, 2.0, 3.0, 2.0, 3.0, 3.0, 2.0, 3.0, 8.0, 2.0, 3.0, 2.0, 5.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 5.0, 5.431639396289985, 3.073511766703173, 5.355035616464652, 2.289845333406147, 2.286824595472415, 5.089737389886066, 3.0473075832396397, 3.0591297248280114, 3.0718326170025088, 3.07183340791106, 3.0718324275444475, 5.895637836551261, 2.109354487858033, 2.109349217733898, 2.1284277667114364, 2.227539429742695, 2.227534137765164, 2.273674217687212, 2.2863810916204796, 2.286384117748805, 2.286382803403265, 2.7524499984085207, 5.281157960398162, 2.884300429268411, 2.983721237284444, 2.996424033457585, 2.9964215390214792, 3.009527060902929, 3.031143691186089, 3.031624114557117, 3.0683820727361857, 5.258151932082869, 3.055663680565296, 3.0649403501306853, 2.2829424023823863, 3.0522286268944043, 3.0683820727361857, 2.10590341089608, 2.124977083465215, 2.2240909696017055, 2.2240908584773567, 2.22408847404067, 2.224086320575112, 2.2584033364560163, 2.258408061573899, 2.2642248884206064, 2.2702306813142896, 2.2702301961249836, 2.2702299468871545, 2.2702305488448977, 2.2702287384752546, 2.270229099693264, 2.286382803403265, 2.286384117748805, 2.2863810916204796, 5.281157960398162, 2.840678272411868, 2.8633593604626495, 2.9964215390214792, 2.996424033457585, 3.009527060902929, 3.0276955152422347, 3.055663680565296, 3.5733871400309454, 3.0718324275444475, 3.07183340791106, 3.690321159642742, 3.0718326170025088, 8.145780221716265, 2.2575330201892854, 2.2575245452058907, 3.0208163259977177, 2.1122751998972884, 2.198284866922418, 2.2113894471629334, 2.2113864850653737, 2.25152294391855, 2.270229099693264, 2.2702287384752546, 2.2702305488448977, 2.2702299468871545, 2.2702301961249836, 2.2702306813142896, 2.273674217687212, 5.258151932082869, 3.0150240418824334, 2.983721237284444, 3.0276955152422347, 3.031143691186089, 3.0522286268944043, 3.055663680565296, 3.0591297248280114, 3.6077515574953964, 3.690321159642742, 3.7132205966975667, 3.760650701265708, 8.145780221716265, 2.286824595472415, 3.073511766703173, 2.851009989480383, 2.1390479891928957, 2.1417530754124865, 2.862193974889044, 3.690321159642742, 2.0339585270943985, 2.053032106696021, 2.0988619129068335, 2.098861742400829, 2.152143003835417, 8.145780221716265, 2.1864625021446327, 2.192270750325903, 2.198284866922418, 2.8163125208659925, 2.884300429268411, 2.955781773613862, 2.983721237284444, 2.9964215390214792, 2.996424033457585, 3.53201120110576, 3.5390863518611417, 3.6077515574953964, 3.7132205966975667, 5.258151932082869, 5.355035616464652, 2.0622655099727973, 2.066134162098878, 2.0813820131847445, 2.087196861519005, 3.5733871400309454, 2.992241436810949, 5.089737389886066, 2.8633593604626495, 2.992241436810949, 2.2338928553958386, 2.233891372429754, 3.760650701265708, 2.2362830895511054, 3.527796252622502, 3.7132205966975667, 2.0813820131847445, 2.1462863204702827, 2.1864625021446327, 2.1995672245775166, 2.2397019609295814, 2.258408061573899, 2.2584033364560163, 2.8633593604626495, 2.9227207575291163, 3.0276955152422347, 3.031143691186089, 3.0473075832396397, 5.281157960398162, 5.895637836551261, 2.0339585270943985, 2.053032106696021, 2.0622655099727973, 2.066134162098878, 2.087196861519005, 2.0988619129068335, 2.098861742400829, 2.10590341089608, 2.109349217733898, 2.1417530754124865, 3.5390863518611417, 3.53201120110576, 5.089737389886066, 3.031624114557117, 3.690321159642742, 3.3600335744723098, 2.192270750325903, 3.0090246934370923, 2.2457386463115823, 3.6137715694366435, 2.955781773613862, 3.0150240418824334, 3.031624114557117, 2.087196861519005, 2.192270750325903, 2.2053833856835396, 2.2397019609295814, 2.25152294391855, 2.2642248884206064, 2.8163125208659925, 3.0208163259977177, 3.53201120110576, 8.145780221716265, 2.2362830895511054, 2.0339585270943985, 2.053032106696021, 2.0622655099727973, 2.066134162098878, 2.0813820131847445, 2.851009989480383, 2.0988619129068335, 2.098861742400829, 2.10590341089608, 2.109349217733898, 2.109354487858033, 2.1122751998972884, 2.124977083465215, 2.1417530754124865, 3.5390863518611417, 5.089737389886066, 3.3600335744723098, 2.992241436810949, 3.690321159642742, 2.8633593604626495, 3.5733871400309454, 2.884300429268411, 5.355035616464652, 2.1284277667114364, 2.7524499984085207, 2.1673698015333973, 2.9227207575291163, 3.5733871400309454, 2.066134162098878, 2.152143003835417, 2.1995672245775166, 2.2053833856835396, 2.2113864850653737, 2.2113894471629334, 2.224086320575112, 2.22408847404067, 2.2240908584773567, 2.2240909696017055, 2.227534137765164, 2.227539429742695, 2.862193974889044, 3.009527060902929, 3.3600335744723098, 3.5390863518611417, 5.258151932082869, 5.895637836551261, 8.145780221716265, 2.0339585270943985, 2.053032106696021, 2.0622655099727973, 2.0813820131847445, 2.087196861519005, 2.098861742400829, 2.0988619129068335, 2.10590341089608, 2.1417530754124865, 2.992241436810949, 3.53201120110576, 5.089737389886066, 2.851009989480383, 2.8633593604626495, 3.690321159642742, 2.884300429268411, 2.0622655099727973, 2.840678272411868, 3.53201120110576, 2.0988619129068335, 2.098861742400829, 2.1462863204702827, 2.884300429268411, 5.895637836551261, 3.3600335744723098, 3.527796252622502, 5.281157960398162, 2.0339585270943985, 2.053032106696021, 2.066134162098878, 2.0813820131847445, 2.087196861519005, 2.10590341089608, 2.109349217733898, 2.109354487858033, 2.1122751998972884, 2.124977083465215, 2.1284277667114364, 2.1390479891928957, 2.1417530754124865, 2.152143003835417, 2.1673698015333973, 2.1864625021446327, 2.192270750325903, 2.198284866922418, 2.1995672245775166, 3.5390863518611417, 2.992241436810949, 3.690321159642742, 3.5733871400309454, 5.089737389886066, 2.851009989480383, 3.031624114557117, 8.145780221716265, 2.8633593604626495, 2.2362830895511054, 5.355035616464652, 5.431639396289985, 2.2575330201892854, 3.7132205966975667, 2.7524499984085207, 3.3600335744723098, 2.053032106696021, 2.066134162098878, 2.1122751998972884, 2.124977083465215, 2.1284277667114364, 2.8163125208659925, 3.527796252622502, 3.5733871400309454, 3.6077515574953964, 5.089737389886066, 2.0339585270943985, 2.0622655099727973, 2.0813820131847445, 2.087196861519005, 2.098861742400829, 2.0988619129068335, 2.10590341089608, 2.109349217733898, 2.109354487858033, 2.1390479891928957, 2.1417530754124865, 2.1462863204702827, 2.152143003835417, 2.1673698015333973, 2.1864625021446327, 2.192270750325903, 2.198284866922418, 2.1995672245775166, 3.5390863518611417, 2.8633593604626495, 2.2362830895511054, 3.53201120110576, 2.992241436810949, 8.145780221716265, 2.851009989480383, 2.884300429268411, 3.690321159642742, 3.031624114557117, 5.281157960398162, 2.2575330201892854, 3.7132205966975667, 2.227534137765164, 2.0339585270943985, 2.0813820131847445, 2.087196861519005, 2.10590341089608, 2.109354487858033, 2.109349217733898, 2.8633593604626495, 3.5390863518611417, 3.6137715694366435, 5.089737389886066, 2.053032106696021, 2.0622655099727973, 2.066134162098878, 2.098861742400829, 2.0988619129068335, 2.1122751998972884, 2.124977083465215, 2.1284277667114364, 2.1390479891928957, 2.1417530754124865, 2.1462863204702827, 2.152143003835417, 2.1673698015333973, 2.1864625021446327, 2.192270750325903, 2.198284866922418, 2.1995672245775166, 2.2053833856835396, 2.2113864850653737, 2.2113894471629334, 2.851009989480383, 3.53201120110576, 3.5733871400309454, 3.690321159642742, 2.992241436810949, 2.2362830895511054, 3.031624114557117, 2.884300429268411, 3.3600335744723098, 8.145780221716265, 2.7524499984085207, 5.355035616464652, 5.431639396289985, 5.281157960398162, 2.2584033364560163, 5.895637836551261], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.4253, -3.1383, -2.619, -3.4917, -3.5847, -2.8226, -3.4917, -3.4917, -3.4917, -3.4917, -3.4917, -3.1022, -4.1383, -4.1383, -4.1383, -4.1383, -4.1383, -4.1383, -4.1383, -4.1383, -4.1383, -4.1384, -3.4917, -4.1383, -4.1383, -4.1383, -4.1383, -4.1383, -4.1383, -4.1384, -4.1383, -4.1383, -4.1383, -2.8658, -3.2552, -3.2552, -3.2552, -3.9019, -3.9019, -3.9019, -3.9019, -3.9019, -3.9019, -3.9019, -3.9019, -3.9019, -3.9019, -3.9019, -3.9019, -3.9019, -3.9019, -3.9019, -3.9019, -3.9019, -3.9019, -3.2552, -3.9019, -3.9019, -3.9019, -3.9019, -3.9019, -3.9019, -3.9018, -3.8697, -3.9019, -3.9019, -3.9019, -3.9019, -3.9019, -3.0166, -3.0166, -3.0166, -3.6632, -3.6632, -3.6633, -3.6633, -3.6633, -3.6632, -3.6632, -3.6632, -3.6633, -3.6633, -3.6633, -3.6632, -3.0166, -3.6311, -3.6632, -3.6633, -3.6633, -3.6632, -3.6633, -3.6632, -3.6632, -3.6632, -3.6632, -3.6633, -3.6311, -5.0086, -5.319, -2.692, -3.0698, -3.0871, -3.0698, -3.0698, -3.7164, -3.7164, -3.7164, -3.7164, -3.7164, -2.4007, -3.7164, -3.7164, -3.7164, -3.7164, -3.7164, -3.7164, -3.7164, -3.7164, -3.7164, -3.7165, -3.7164, -3.7164, -3.7164, -3.7164, -3.7164, -6.1143, -6.1143, -6.1143, -6.1143, -6.1143, -6.1143, -6.1143, -6.1143, -2.5654, -2.9432, -2.9432, -2.5537, -3.1626, -2.9432, -2.9432, -3.5898, -3.5898, -3.5898, -3.5898, -3.5898, -3.5898, -3.5898, -3.5898, -3.5898, -3.5898, -3.5898, -3.5898, -3.5898, -3.5898, -5.9877, -5.9877, -5.9877, -5.9877, -5.9877, -5.9877, -5.9877, -5.9877, -5.9877, -5.9877, -5.9877, -5.9877, -5.9877, -5.9877, -5.9877, -5.9877, -5.9877, -2.4508, -2.8458, -2.4391, -2.8458, -2.8458, -2.8458, -3.4752, -3.4752, -3.4752, -3.4752, -3.4752, -3.4752, -3.4752, -3.4752, -3.4752, -2.8458, -4.2367, -5.8731, -5.8731, -5.8731, -5.8731, -5.8731, -5.5659, -5.8731, -5.8731, -5.8731, -5.8731, -5.8731, -5.8731, -5.8731, -5.873, -5.873, -5.873, -5.873, -5.873, -5.873, -5.8731, -5.8731, -5.8731, -5.8731, -5.8731, -5.8731, -2.8372, -2.8199, -2.8372, -3.4665, -3.4665, -3.4665, -3.4665, -3.4665, -3.4665, -3.4665, -3.4665, -3.4665, -3.4665, -3.4665, -3.4665, -3.4666, -3.4665, -3.4666, -3.4665, -3.4665, -3.4666, -3.4665, -5.8644, -5.8644, -5.8644, -5.8644, -5.8644, -5.8644, -5.8644, -5.8644, -5.8644, -5.8644, -5.8644, -5.8644, -5.8644, -5.8644, -5.8644, -5.8644, -2.5047, -2.4874, -2.4874, -3.1341, -3.1341, -3.1341, -3.1341, -2.4874, -3.1341, -3.1341, -3.1341, -5.532, -5.532, -5.532, -5.532, -5.532, -5.532, -5.532, -5.532, -5.532, -5.532, -5.532, -5.532, -5.5319, -5.532, -5.532, -5.532, -5.532, -5.532, -5.532, -5.5319, -5.532, -5.532, -5.532, -5.532, -5.532, -5.532, -5.532, -5.532, -5.532, -5.532, -5.532, -5.532, -5.532, -2.4485, -2.4485, -3.0952, -3.0952, -3.0952, -3.0952, -3.0952, -3.0952, -3.0952, -3.0952, -3.0952, -3.0952, -5.4931, -5.493, -5.4931, -5.4931, -5.4931, -5.4931, -5.4931, -5.493, -5.4931, -5.4931, -5.493, -5.4931, -5.4931, -5.4931, -5.4931, -5.493, -5.4931, -5.4931, -5.493, -5.493, -5.493, -5.493, -5.493, -5.493, -5.493, -5.493, -5.493, -5.493, -5.493, -5.493, -5.493, -5.4931, -2.9153, -2.9153, -2.9153, -2.9153, -2.9153, -2.9153, -2.9153, -2.9153, -2.9153, -2.9153, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.5489, 1.4053, 1.3694, 1.3463, 1.2546, 1.2166, 1.0605, 1.0566, 1.0525, 1.0525, 1.0525, 0.79, 0.7817, 0.7817, 0.7728, 0.7272, 0.7272, 0.7067, 0.7012, 0.7012, 0.7012, 0.5156, 0.5106, 0.4689, 0.4349, 0.4307, 0.4307, 0.4263, 0.4192, 0.419, 0.407, -0.1317, 0.4111, 1.6806, 1.5857, 1.2953, 1.2901, 1.0198, 1.0108, 0.9652, 0.9652, 0.9652, 0.9652, 0.9499, 0.9499, 0.9473, 0.9447, 0.9447, 0.9447, 0.9447, 0.9447, 0.9447, 0.9376, 0.9376, 0.9376, 0.7471, 0.7205, 0.7126, 0.6671, 0.6671, 0.6628, 0.6568, 0.6476, 0.5232, 0.6423, 0.6423, 0.4589, 0.6423, -0.3329, 1.8355, 1.8355, 1.5443, 1.2554, 1.2155, 1.2096, 1.2095, 1.1916, 1.1833, 1.1833, 1.1833, 1.1833, 1.1833, 1.1833, 1.1818, 0.99, 0.9317, 0.91, 0.8954, 0.8942, 0.8873, 0.8862, 0.8851, 0.7201, 0.6975, 0.6913, 0.6786, -0.0621, -0.1694, -0.7754, 1.9268, 1.8363, 1.8177, 1.5451, 1.2909, 1.24, 1.2307, 1.2086, 1.2086, 1.1835, 1.1682, 1.1677, 1.1651, 1.1623, 0.9146, 0.8907, 0.8662, 0.8569, 0.8526, 0.8526, 0.6881, 0.6862, 0.6669, 0.6381, 0.2902, 0.2719, -1.1717, -1.1735, -1.1809, -1.1837, -1.7214, -1.5439, -2.0751, -1.4998, 2.005, 1.9195, 1.9195, 1.7881, 1.699, 1.4626, 1.4113, 1.3436, 1.3129, 1.2943, 1.2883, 1.2703, 1.262, 1.2619, 1.0246, 1.0041, 0.9688, 0.9677, 0.9623, 0.4125, 0.3024, -1.0313, -1.0406, -1.0451, -1.047, -1.0571, -1.0627, -1.0627, -1.066, -1.0677, -1.0829, -1.5851, -1.5831, -1.9485, -1.4304, -1.627, -1.5332, -1.1062, 2.1141, 2.0116, 1.9426, 1.7368, 1.717, 1.7115, 1.4554, 1.4063, 1.4003, 1.3849, 1.3796, 1.374, 1.1558, 1.0857, 0.9294, 0.7231, 0.6249, -0.9166, -0.926, -0.9304, -0.9323, -0.9397, -0.9471, -0.948, -0.948, -0.9514, -0.953, -0.953, -0.9544, -0.9604, -0.9682, -1.4705, -1.8338, -1.4186, -1.3026, -1.5123, -1.2586, -1.4801, -1.2659, -1.8847, -0.962, -1.2191, 2.0557, 1.774, 1.5557, 1.4742, 1.4334, 1.4116, 1.409, 1.4063, 1.4063, 1.4006, 1.4006, 1.4005, 1.4005, 1.399, 1.399, 1.1483, 1.0981, 0.9879, 0.936, 0.5401, 0.4256, 0.1024, -0.908, -0.9173, -0.9218, -0.931, -0.9338, -0.9394, -0.9394, -0.9427, -0.9596, -1.294, -1.4598, -1.8252, -1.2456, -1.25, -1.5037, -1.2573, 2.4379, 2.1349, 1.9171, 1.791, 1.791, 1.7686, 1.473, 1.4048, 1.3204, 1.2717, 0.8682, -0.5755, -0.5849, -0.5912, -0.5986, -0.6014, -0.6103, -0.6119, -0.6119, -0.6133, -0.6193, -0.6209, -0.6259, -0.6271, -0.632, -0.639, -0.6478, -0.6505, -0.6532, -0.6538, -1.1294, -0.9615, -1.1712, -1.139, -1.4928, -0.9132, -0.9746, -1.963, -0.9175, -0.6703, -1.5436, -1.5578, -0.6798, -1.1774, 2.2054, 2.0059, 1.8519, 1.8456, 1.8235, 1.8175, 1.8158, 1.5358, 1.3105, 1.2977, 1.2882, 0.944, -0.5366, -0.5504, -0.5597, -0.5625, -0.568, -0.568, -0.5714, -0.573, -0.573, -0.587, -0.5882, -0.5904, -0.5931, -0.6001, -0.6089, -0.6116, -0.6143, -0.6149, -1.0905, -0.8786, -0.6314, -1.0885, -0.9226, -1.9241, -0.8743, -0.8859, -1.1323, -0.9357, -1.4908, -0.6409, -1.1385, -0.6275, 2.0411, 2.0181, 2.0153, 2.0064, 2.0047, 2.0047, 1.6991, 1.4872, 1.4663, 1.1239, -0.3661, -0.3706, -0.3724, -0.3882, -0.3882, -0.3945, -0.4005, -0.4021, -0.4071, -0.4084, -0.4105, -0.4132, -0.4203, -0.4291, -0.4317, -0.4345, -0.435, -0.4377, -0.4404, -0.4404, -0.6944, -0.9086, -0.9203, -0.9525, -0.7428, -0.4516, -0.7559, -0.706, -0.8587, -1.7443, -0.6593, -1.3248, -1.339, -1.3109, -0.4614, -1.421]}, \"token.table\": {\"Topic\": [4, 6, 9, 2, 3, 1, 5, 7, 8, 1, 3, 4, 1, 2, 1, 2, 3, 4, 6, 7, 5, 8, 9, 4, 1, 2, 4, 6, 10, 5, 10, 4, 6, 2, 3, 4, 8, 1, 2, 1, 4, 8, 3, 4, 5, 4, 7, 4, 1, 2, 6, 7, 1, 1, 5, 5, 1, 9, 10, 5, 8, 2, 3, 5, 3, 6, 1, 3, 5, 5, 2, 3, 6, 3, 5, 2, 7, 4, 1, 2, 3, 2, 8, 2, 9, 3, 7, 2, 3, 1, 2, 4, 9, 4, 5, 2, 7, 8, 5, 7, 1, 2, 2, 5, 4, 7, 3, 1, 10, 4, 6, 5, 1, 2, 3, 3, 7, 2, 5, 1, 9, 3, 9, 1, 2, 2, 1, 2, 6, 2, 3, 2, 3, 3, 6, 1, 4, 2, 7, 1, 3, 3, 6, 2, 7, 9, 1, 3, 1, 2, 7, 5, 6, 7, 4, 10, 1, 7, 6, 2, 7, 1, 4, 8, 6, 10, 1, 10, 2, 5, 10, 1, 2, 5, 8, 2, 5, 1, 2, 3, 2, 3, 4, 1, 2, 4, 2, 4, 7, 10, 1, 9, 1, 2, 3, 4, 7, 3, 4, 7, 8, 9, 5, 7, 1, 7, 7, 9, 1, 6, 4, 6, 8, 1, 3, 4, 9, 2, 10], \"Freq\": [0.3550742300760387, 0.3550742300760387, 0.3550742300760387, 0.44048389733316023, 0.44048389733316023, 0.3392338633151064, 0.1696169316575532, 0.1696169316575532, 0.1696169316575532, 0.3351519530390593, 0.3351519530390593, 0.3351519530390593, 0.4373718275232813, 0.4373718275232813, 0.12276294876383324, 0.12276294876383324, 0.12276294876383324, 0.3682888462914997, 0.2455258975276665, 0.12276294876383324, 0.5669261648864315, 0.2834630824432158, 0.2834630824432158, 0.4674976929233455, 0.3337311371268426, 0.3337311371268426, 0.3337311371268426, 0.4791114908405081, 0.4791114908405081, 0.4804500056526815, 0.4804500056526815, 0.4561480373039416, 0.4561480373039416, 0.4404841785065271, 0.4404841785065271, 0.4764486857618198, 0.4764486857618198, 0.43737207894999336, 0.43737207894999336, 0.8734214362963102, 0.4764487244672572, 0.4764487244672572, 0.2693079966456536, 0.2693079966456536, 0.5386159932913072, 0.3493823300493692, 0.3493823300493692, 0.4669072319680956, 0.325904654731692, 0.651809309463384, 0.45343589984924937, 0.45343589984924937, 0.8745751659133426, 0.6563170751125061, 0.32815853755625307, 0.8952980870005095, 0.5894213728907446, 0.19647379096358153, 0.19647379096358153, 0.465921061166194, 0.465921061166194, 0.33028420294105887, 0.33028420294105887, 0.33028420294105887, 0.3316723137556306, 0.6633446275112612, 0.3299084774198544, 0.3299084774198544, 0.3299084774198544, 0.6683952622925864, 0.4404840141286827, 0.4404840141286827, 0.8905755811277571, 0.2659114284832233, 0.5318228569664466, 0.44962191908140564, 0.44962191908140564, 0.7015057847498157, 0.3272611466897432, 0.3272611466897432, 0.3272611466897432, 0.35202860165891064, 0.35202860165891064, 0.470593310290807, 0.470593310290807, 0.45220498847827484, 0.45220498847827484, 0.655258908974643, 0.3276294544873215, 0.6510771416808504, 0.3255385708404252, 0.4870844429263782, 0.4870844429263782, 0.45735977590245946, 0.45735977590245946, 0.4496228364650058, 0.4496228364650058, 0.4849036145754048, 0.34214695243257015, 0.6842939048651403, 0.6510771818366259, 0.32553859091831294, 0.442789776132438, 0.442789776132438, 0.46465313792710866, 0.46465313792710866, 0.8859261372139792, 0.474078684145433, 0.474078684145433, 0.33831996966993894, 0.6766399393398779, 0.8952986813430612, 0.9205323909048875, 0.44048396577002746, 0.44048396577002746, 0.4522043827616768, 0.4522043827616768, 0.652541247634631, 0.447170577228097, 0.363312685272468, 0.363312685272468, 0.47342316003550394, 0.47342316003550394, 0.4373724064045889, 0.4373724064045889, 0.8760623999593161, 0.6507214391260705, 0.44165224272291376, 0.44165224272291376, 0.44048387163064706, 0.44048387163064706, 0.44048424859233626, 0.44048424859233626, 0.4441438194982816, 0.4441438194982816, 0.7469604847634543, 0.18674012119086358, 0.4496218966165228, 0.4496218966165228, 0.43981674780884084, 0.43981674780884084, 0.6620726929961352, 0.3310363464980676, 0.27984653238309354, 0.27984653238309354, 0.27984653238309354, 0.6537807088623686, 0.3268903544311843, 0.33227812203156476, 0.33227812203156476, 0.33227812203156476, 0.4464879780633639, 0.4464879780633639, 0.4613887299216349, 0.49165210926328234, 0.49165210926328234, 0.4489258356766824, 0.4489258356766824, 0.6646671941118162, 0.4496224011193333, 0.4496224011193333, 0.3467045214335197, 0.3467045214335197, 0.3467045214335197, 0.5534384123542659, 0.27671920617713297, 0.47407986861194723, 0.47407986861194723, 0.3492401316467746, 0.3492401316467746, 0.3492401316467746, 0.3787048247746814, 0.3787048247746814, 0.1893524123873407, 0.1893524123873407, 0.4427907025541518, 0.4427907025541518, 0.651076974047255, 0.3255384870236275, 0.8859228113670328, 0.27097912532274276, 0.27097912532274276, 0.27097912532274276, 0.3337314149485667, 0.3337314149485667, 0.3337314149485667, 0.2825588020688215, 0.2825588020688215, 0.2825588020688215, 0.2825588020688215, 0.4698303675792893, 0.4698303675792893, 0.1901808872996711, 0.1901808872996711, 0.3803617745993422, 0.1901808872996711, 0.1901808872996711, 0.4549000973654485, 0.4549000973654485, 0.29761607371945653, 0.29761607371945653, 0.29761607371945653, 0.45463488854816686, 0.45463488854816686, 0.4489269021947641, 0.4489269021947641, 0.483995675762, 0.483995675762, 0.32985619661693705, 0.6597123932338741, 0.28312480993461514, 0.28312480993461514, 0.28312480993461514, 0.27718094887172007, 0.27718094887172007, 0.27718094887172007, 0.27718094887172007, 0.4748555868355289, 0.4748555868355289], \"Term\": [\"10\", \"10\", \"10\", \"account\", \"account\", \"actually\", \"actually\", \"actually\", \"actually\", \"ago\", \"ago\", \"ago\", \"alcohol\", \"alcohol\", \"amp\", \"amp\", \"amp\", \"amp\", \"amp\", \"amp\", \"bbc\", \"bbc\", \"bbc\", \"billym2k\", \"buy\", \"buy\", \"buy\", \"california\", \"california\", \"case\", \"case\", \"cbdoge\", \"cbdoge\", \"check\", \"check\", \"company\", \"company\", \"crime\", \"crime\", \"criminal\", \"davidasinclair\", \"davidasinclair\", \"davidmweissman\", \"davidmweissman\", \"davidmweissman\", \"day\", \"day\", \"dogeofficialceo\", \"drug\", \"drug\", \"end\", \"end\", \"fix\", \"friend\", \"friend\", \"fund\", \"good\", \"good\", \"good\", \"great\", \"great\", \"greatly\", \"greatly\", \"greatly\", \"heavy\", \"heavy\", \"help\", \"help\", \"help\", \"iamharaldur\", \"importantly\", \"importantly\", \"industry\", \"influence\", \"influence\", \"investmattallen\", \"investmattallen\", \"jonerlichman\", \"kanekoathegreat\", \"kanekoathegreat\", \"kanekoathegreat\", \"know\", \"know\", \"lack\", \"lack\", \"launch\", \"launch\", \"legacy\", \"legacy\", \"let\", \"let\", \"live\", \"live\", \"lol\", \"lol\", \"long\", \"long\", \"matter\", \"medium\", \"medium\", \"micsolana\", \"micsolana\", \"need\", \"need\", \"oh\", \"oh\", \"old\", \"open\", \"open\", \"paulg\", \"paulg\", \"pay\", \"people\", \"platform\", \"platform\", \"post\", \"post\", \"press\", \"principle\", \"probably\", \"probably\", \"problem\", \"problem\", \"propaganda\", \"propaganda\", \"public\", \"question\", \"rapidly\", \"rapidly\", \"reduce\", \"reduce\", \"remove\", \"remove\", \"require\", \"require\", \"right\", \"right\", \"robbysoave\", \"robbysoave\", \"rt\", \"rt\", \"say\", \"say\", \"scottadamssays\", \"scottadamssays\", \"scottadamssays\", \"sf\", \"sf\", \"shellenbergermd\", \"shellenbergermd\", \"shellenbergermd\", \"silly\", \"silly\", \"starship\", \"supplement\", \"supplement\", \"take\", \"take\", \"talent\", \"tbh\", \"tbh\", \"tell\", \"tell\", \"tell\", \"tesla\", \"tesla\", \"teslaownerssv\", \"teslaownerssv\", \"therabbithole84\", \"therabbithole84\", \"therabbithole84\", \"think\", \"think\", \"think\", \"think\", \"thinking\", \"thinking\", \"time\", \"time\", \"timsweeneyepic\", \"titterdaily\", \"titterdaily\", \"titterdaily\", \"true\", \"true\", \"true\", \"trungtphan\", \"trungtphan\", \"trungtphan\", \"trungtphan\", \"try\", \"try\", \"twitter\", \"twitter\", \"twitter\", \"twitter\", \"twitter\", \"w\", \"w\", \"wallstreetsilv\", \"wallstreetsilv\", \"wallstreetsilv\", \"way\", \"way\", \"world\", \"world\", \"wow\", \"wow\", \"wrong\", \"wrong\", \"yeah\", \"yeah\", \"yeah\", \"year\", \"year\", \"year\", \"year\", \"yes\", \"yes\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [5, 4, 1, 8, 3, 7, 9, 2, 6, 10]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el684140097772805248710964983\", ldavis_el684140097772805248710964983_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el684140097772805248710964983\", ldavis_el684140097772805248710964983_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el684140097772805248710964983\", ldavis_el684140097772805248710964983_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "A5cnjbQZ3olQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "data_vectorized = vectorizer.fit_transform(lemmas_strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luaWtB2D3sWu",
        "outputId": "e6eb2404-e17a-4254-ddfc-2425caa8505e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Search Param\n",
        "search_params = {'n_components': [10, 11, 12, 13, 14, 15], 'learning_decay': [.5, .7, .9]}\n",
        "\n",
        "# Init the Model\n",
        "lda = LatentDirichletAllocation()\n",
        "\n",
        "# Init Grid Search Class\n",
        "model = GridSearchCV(lda, param_grid=search_params)\n",
        "\n",
        "# Do the Grid Search\n",
        "model.fit(data_vectorized)\n",
        "GridSearchCV(cv=None, error_score='raise',\n",
        "             estimator=LatentDirichletAllocation(batch_size=128, \n",
        "                                                 doc_topic_prior=None,\n",
        "                                                 evaluate_every=-1, \n",
        "                                                 learning_decay=0.7, \n",
        "                                                 learning_method=None,\n",
        "                                                 learning_offset=10.0, \n",
        "                                                 max_doc_update_iter=100, \n",
        "                                                 max_iter=10,\n",
        "                                                 mean_change_tol=0.001, \n",
        "                                                 n_components=10, \n",
        "                                                 n_jobs=1,\n",
        "                                                 perp_tol=0.1, \n",
        "                                                 random_state=None,\n",
        "                                                 topic_word_prior=None, \n",
        "                                                 total_samples=1000000.0, \n",
        "                                                 verbose=0),\n",
        "             n_jobs=1,\n",
        "             param_grid={'n_topics': [10, 15, 20, 30], \n",
        "                         'learning_decay': [0.5, 0.7, 0.9]},\n",
        "             pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
        "             scoring=None, verbose=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "WaRZ6H7W4HW_",
        "outputId": "17387dd0-5484-4387-c649-c558c102c543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(error_score='raise',\n",
              "             estimator=LatentDirichletAllocation(learning_method=None,\n",
              "                                                 n_jobs=1),\n",
              "             n_jobs=1,\n",
              "             param_grid={'learning_decay': [0.5, 0.7, 0.9],\n",
              "                         'n_topics': [10, 15, 20, 30]},\n",
              "             return_train_score='warn')"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(error_score=&#x27;raise&#x27;,\n",
              "             estimator=LatentDirichletAllocation(learning_method=None,\n",
              "                                                 n_jobs=1),\n",
              "             n_jobs=1,\n",
              "             param_grid={&#x27;learning_decay&#x27;: [0.5, 0.7, 0.9],\n",
              "                         &#x27;n_topics&#x27;: [10, 15, 20, 30]},\n",
              "             return_train_score=&#x27;warn&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(error_score=&#x27;raise&#x27;,\n",
              "             estimator=LatentDirichletAllocation(learning_method=None,\n",
              "                                                 n_jobs=1),\n",
              "             n_jobs=1,\n",
              "             param_grid={&#x27;learning_decay&#x27;: [0.5, 0.7, 0.9],\n",
              "                         &#x27;n_topics&#x27;: [10, 15, 20, 30]},\n",
              "             return_train_score=&#x27;warn&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(learning_method=None, n_jobs=1)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(learning_method=None, n_jobs=1)</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Model\n",
        "best_lda_model = model.best_estimator_\n",
        "\n",
        "# Model Parameters\n",
        "print(\"Best Model's Params: \", model.best_params_)\n",
        "\n",
        "# Log Likelihood Score\n",
        "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
        "\n",
        "# Perplexity\n",
        "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_3BJQsC5ZNZ",
        "outputId": "3caa93a0-397c-48d5-ed9c-4b0e1f51926b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model's Params:  {'learning_decay': 0.5, 'n_components': 10}\n",
            "Best Log Likelihood Score:  -2681.1105152334885\n",
            "Model Perplexity:  1158.7739884348007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding Best number of Topics"
      ],
      "metadata": {
        "id": "ABxZzkOl6UYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_list_topic, coherence_values_topic = compute_coherence_values(dictionary=id2word,\n",
        "                                                        corpus=corpus,\n",
        "                                                        texts=tweets_df['final_tokens'],\n",
        "                                                        start=10, limit=16, step=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXxU5Yl66Xfe",
        "outputId": "d9081b2f-a5aa-4353-cfaf-4b6812748d1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot([10,11,12,13,14,15], coherence_values_topic)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "zr3p21FI7Pij",
        "outputId": "8fcba445-4abb-4291-baad-a5a4d7adfd86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f6add5fdf10>]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_model = LdaMulticore(corpus=corpus,\n",
        "                       id2word=id2word,\n",
        "                       num_topics=10,\n",
        "                       random_state=42,\n",
        "                       chunksize=2000,\n",
        "                       passes=25,\n",
        "                       decay=0.5,\n",
        "                       iterations=70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TTKQCmj7lys",
        "outputId": "1a5ecd38-5191-4c38-f132-c66d3a098dc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_words = [re.findall(r'\"([^\"]*)\"',t[1]) for t in final_model.print_topics()]\n",
        "\n",
        "\n",
        "final_topics = [' '.join(t[0:10]) for t in final_words]\n",
        "\n",
        "for id, t in enumerate(final_topics): \n",
        "    print(f\"------ Topic {id} ------\")\n",
        "    print(t, end=\"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_R54OES9-Ln",
        "outputId": "9f4fbf6f-3641-4812-eb04-cd51e7bdb415"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------ Topic 0 ------\n",
            "iamharaldur lol friend question great people way shellenbergermd good thinking\n",
            "\n",
            "------ Topic 1 ------\n",
            "bbc wallstreetsilv actually influence trungtphan yes think case fund need\n",
            "\n",
            "------ Topic 2 ------\n",
            "talent jonerlichman right heavy paulg industry cbdoge old 10 fix\n",
            "\n",
            "------ Topic 3 ------\n",
            "think probably scottadamssays drug time good legacy micsolana crime greatly\n",
            "\n",
            "------ Topic 4 ------\n",
            "amp day davidmweissman year timsweeneyepic teslaownerssv principle titterdaily check legacy\n",
            "\n",
            "------ Topic 5 ------\n",
            "yeah tesla people know right press actually davidasinclair try sf\n",
            "\n",
            "------ Topic 6 ------\n",
            "good help let platform titterdaily kanekoathegreat tell friend propaganda criminal\n",
            "\n",
            "------ Topic 7 ------\n",
            "therabbithole84 dogeofficialceo thinking pay long micsolana right trungtphan iamharaldur lol\n",
            "\n",
            "------ Topic 8 ------\n",
            "true twitter year ago actually scottadamssays buy davidmweissman 10 medium\n",
            "\n",
            "------ Topic 9 ------\n",
            "amp w launch rt wrong tell billym2k problem remove sf\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_perplexity = final_model.log_perplexity(corpus)\n",
        "print('\\nPerplexity: ', final_perplexity) \n",
        "\n",
        "final_coherence_model = CoherenceModel(model=final_model, texts=tweets_df['final_tokens'], \n",
        "                                   dictionary=id2word, coherence='c_v')\n",
        "coherence_lda_model_final = final_coherence_model.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda_model_final)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BADUsUax-S3n",
        "outputId": "6e96f952-0101-4517-e8eb-caa1c4b8afa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Perplexity:  -5.697837825562185\n",
            "\n",
            "Coherence Score:  0.5292507590703907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Topic Distance Visualization \n",
        "pyLDAvis.enable_notebook()\n",
        "pyLDAvis.gensim.prepare(final_model, corpus, id2word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 951
        },
        "id": "dpy0PSUn-otu",
        "outputId": "7bbaa23c-647d-4293-cf64-b14534860140"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.9/dist-packages/pyLDAvis/_prepare.py:243: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
            "  default_term_info = default_term_info.sort_values(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
              "topic                                                \n",
              "5      0.106937  0.015167       1        1  16.965980\n",
              "1     -0.053677 -0.221501       2        1  13.118160\n",
              "4     -0.149146  0.069362       3        1  12.145060\n",
              "8     -0.089572 -0.057599       4        1  11.914601\n",
              "3      0.152725  0.038128       5        1   9.756864\n",
              "6      0.127485  0.003126       6        1   9.256536\n",
              "9     -0.027388  0.019354       7        1   9.074430\n",
              "2     -0.108133  0.121512       8        1   8.704397\n",
              "0      0.044594 -0.017141       9        1   4.941456\n",
              "7     -0.003825  0.029592      10        1   4.122516, topic_info=                Term      Freq     Total Category  logprob  loglift\n",
              "24       iamharaldur  2.000000  2.000000  Default  30.0000  30.0000\n",
              "27              good  5.000000  5.000000  Default  29.0000  29.0000\n",
              "90   therabbithole84  2.000000  2.000000  Default  28.0000  28.0000\n",
              "1              right  5.000000  5.000000  Default  27.0000  27.0000\n",
              "102           talent  2.000000  2.000000  Default  26.0000  26.0000\n",
              "..               ...       ...       ...      ...      ...      ...\n",
              "47             think  0.058689  5.301867  Topic10  -5.3132  -1.3149\n",
              "57          probably  0.058689  2.930590  Topic10  -5.3132  -0.7220\n",
              "61    davidmweissman  0.058689  3.766931  Topic10  -5.3132  -0.9731\n",
              "64      jonerlichman  0.058689  2.907160  Topic10  -5.3132  -0.7140\n",
              "66           company  0.058689  2.291360  Topic10  -5.3132  -0.4759\n",
              "\n",
              "[411 rows x 6 columns], token_table=      Topic      Freq     Term\n",
              "term                          \n",
              "59        3  0.341027       10\n",
              "59        4  0.341027       10\n",
              "59        8  0.341027       10\n",
              "85        3  0.455259  account\n",
              "85        6  0.455259  account\n",
              "...     ...       ...      ...\n",
              "5         1  0.767229     yeah\n",
              "5         3  0.255743     yeah\n",
              "63        3  0.537984     year\n",
              "63        4  0.537984     year\n",
              "38        2  0.898239      yes\n",
              "\n",
              "[211 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[6, 2, 5, 9, 4, 7, 10, 3, 1, 8])"
            ],
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el6841400969619949288016657665\" style=\"background-color:white;\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el6841400969619949288016657665_data = {\"mdsDat\": {\"x\": [0.10693699701374394, -0.05367658126855417, -0.14914579012852502, -0.08957201656963022, 0.15272480427031876, 0.12748532517131722, -0.027388309341514948, -0.1081329705630773, 0.044593659181950206, -0.0038251177660280585], \"y\": [0.015167362874476795, -0.2215008968255253, 0.06936209939484098, -0.05759864835004817, 0.03812800789037364, 0.003125681630626236, 0.019353919193833648, 0.12151193422385485, -0.017141157063285523, 0.029591697030852585], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [16.965980217362162, 13.118160057430398, 12.14506016722025, 11.914600952953759, 9.75686358543011, 9.256535988886034, 9.074430086486643, 8.704396720455392, 4.941456332957631, 4.122515890817621]}, \"tinfo\": {\"Term\": [\"iamharaldur\", \"good\", \"therabbithole84\", \"right\", \"talent\", \"dogeofficialceo\", \"thinking\", \"bbc\", \"wallstreetsilv\", \"scottadamssays\", \"think\", \"probably\", \"amp\", \"people\", \"true\", \"actually\", \"micsolana\", \"question\", \"friend\", \"trungtphan\", \"day\", \"heavy\", \"titterdaily\", \"tell\", \"let\", \"jonerlichman\", \"paulg\", \"shellenbergermd\", \"long\", \"influence\", \"know\", \"davidasinclair\", \"try\", \"yeah\", \"tesla\", \"sf\", \"press\", \"people\", \"pay\", \"lol\", \"lack\", \"end\", \"require\", \"open\", \"california\", \"alcohol\", \"matter\", \"rapidly\", \"fund\", \"company\", \"public\", \"right\", \"think\", \"micsolana\", \"wrong\", \"say\", \"time\", \"drug\", \"help\", \"kanekoathegreat\", \"actually\", \"buy\", \"twitter\", \"bbc\", \"wallstreetsilv\", \"yes\", \"trungtphan\", \"influence\", \"great\", \"actually\", \"silly\", \"oh\", \"old\", \"world\", \"problem\", \"case\", \"importantly\", \"wow\", \"tbh\", \"need\", \"investmattallen\", \"robbysoave\", \"fund\", \"shellenbergermd\", \"greatly\", \"medium\", \"kanekoathegreat\", \"buy\", \"twitter\", \"think\", \"dogeofficialceo\", \"thinking\", \"long\", \"yeah\", \"know\", \"good\", \"day\", \"timsweeneyepic\", \"teslaownerssv\", \"principle\", \"year\", \"davidmweissman\", \"way\", \"live\", \"fix\", \"supplement\", \"account\", \"tbh\", \"wow\", \"check\", \"post\", \"take\", \"amp\", \"paulg\", \"10\", \"legacy\", \"medium\", \"ago\", \"say\", \"billym2k\", \"titterdaily\", \"influence\", \"right\", \"twitter\", \"yeah\", \"dogeofficialceo\", \"scottadamssays\", \"true\", \"ago\", \"year\", \"long\", \"remove\", \"crime\", \"criminal\", \"propaganda\", \"take\", \"post\", \"check\", \"investmattallen\", \"robbysoave\", \"need\", \"public\", \"company\", \"therabbithole84\", \"twitter\", \"shellenbergermd\", \"jonerlichman\", \"10\", \"legacy\", \"medium\", \"buy\", \"actually\", \"scottadamssays\", \"davidmweissman\", \"people\", \"amp\", \"thinking\", \"probably\", \"probably\", \"scottadamssays\", \"drug\", \"time\", \"think\", \"starship\", \"reduce\", \"supplement\", \"crime\", \"alcohol\", \"california\", \"open\", \"good\", \"micsolana\", \"let\", \"greatly\", \"legacy\", \"tesla\", \"amp\", \"dogeofficialceo\", \"thinking\", \"long\", \"pay\", \"way\", \"great\", \"cbdoge\", \"industry\", \"rt\", \"launch\", \"w\", \"day\", \"yeah\", \"actually\", \"wrong\", \"iamharaldur\", \"matter\", \"yes\", \"billym2k\", \"jonerlichman\", \"principle\", \"platform\", \"let\", \"help\", \"good\", \"titterdaily\", \"reduce\", \"account\", \"importantly\", \"case\", \"propaganda\", \"criminal\", \"rapidly\", \"matter\", \"question\", \"friend\", \"tell\", \"greatly\", \"kanekoathegreat\", \"press\", \"people\", \"dogeofficialceo\", \"thinking\", \"long\", \"pay\", \"way\", \"great\", \"cbdoge\", \"industry\", \"rt\", \"launch\", \"iamharaldur\", \"right\", \"yeah\", \"actually\", \"try\", \"wrong\", \"yes\", \"amp\", \"billym2k\", \"probably\", \"w\", \"launch\", \"rt\", \"tell\", \"wrong\", \"billym2k\", \"starship\", \"problem\", \"world\", \"remove\", \"require\", \"end\", \"friend\", \"heavy\", \"amp\", \"say\", \"sf\", \"trungtphan\", \"twitter\", \"dogeofficialceo\", \"thinking\", \"long\", \"pay\", \"way\", \"great\", \"cbdoge\", \"industry\", \"live\", \"fix\", \"old\", \"titterdaily\", \"yeah\", \"right\", \"actually\", \"try\", \"iamharaldur\", \"good\", \"kanekoathegreat\", \"matter\", \"yes\", \"talent\", \"industry\", \"cbdoge\", \"heavy\", \"paulg\", \"jonerlichman\", \"fix\", \"live\", \"old\", \"oh\", \"silly\", \"lack\", \"right\", \"question\", \"10\", \"titterdaily\", \"amp\", \"dogeofficialceo\", \"thinking\", \"long\", \"pay\", \"way\", \"great\", \"rt\", \"launch\", \"w\", \"billym2k\", \"starship\", \"problem\", \"world\", \"yeah\", \"actually\", \"try\", \"wrong\", \"iamharaldur\", \"good\", \"matter\", \"principle\", \"therabbithole84\", \"tell\", \"trungtphan\", \"iamharaldur\", \"way\", \"great\", \"lol\", \"question\", \"friend\", \"shellenbergermd\", \"good\", \"people\", \"thinking\", \"dogeofficialceo\", \"long\", \"pay\", \"cbdoge\", \"industry\", \"rt\", \"launch\", \"w\", \"billym2k\", \"live\", \"fix\", \"starship\", \"old\", \"oh\", \"silly\", \"problem\", \"world\", \"reduce\", \"platform\", \"supplement\", \"right\", \"yeah\", \"actually\", \"try\", \"wrong\", \"kanekoathegreat\", \"matter\", \"yes\", \"amp\", \"probably\", \"davidmweissman\", \"jonerlichman\", \"principle\", \"teslaownerssv\", \"dogeofficialceo\", \"thinking\", \"therabbithole84\", \"long\", \"pay\", \"micsolana\", \"right\", \"way\", \"great\", \"cbdoge\", \"industry\", \"rt\", \"launch\", \"w\", \"billym2k\", \"live\", \"fix\", \"starship\", \"old\", \"oh\", \"silly\", \"problem\", \"world\", \"reduce\", \"platform\", \"supplement\", \"account\", \"remove\", \"principle\", \"timsweeneyepic\", \"trungtphan\", \"iamharaldur\", \"yeah\", \"actually\", \"people\", \"try\", \"friend\", \"wrong\", \"good\", \"kanekoathegreat\", \"matter\", \"yes\", \"amp\", \"think\", \"probably\", \"davidmweissman\", \"jonerlichman\", \"company\"], \"Freq\": [2.0, 5.0, 2.0, 5.0, 2.0, 1.0, 1.0, 3.0, 3.0, 3.0, 5.0, 2.0, 8.0, 5.0, 3.0, 6.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.4909853684401826, 1.6998533322645861, 1.699850957720294, 2.7491883830086663, 2.5093285404387884, 1.6998473045752294, 1.699856437437891, 2.509319224918874, 0.8903783980762633, 0.8903501775306389, 0.890397029116093, 0.8903966638015866, 0.8903931019851485, 0.8903964811443333, 0.8903955678580672, 0.8903938326141615, 0.8904092671520596, 0.8903937412855348, 0.8903872569530451, 0.8903996776462649, 0.8903964811443333, 1.6998785389655322, 1.699843103458405, 0.890393467299655, 0.8903980337309858, 0.8903920060416292, 0.890391092755363, 0.8903892661828307, 0.8903679866128291, 0.8904022348478101, 1.6998538802363456, 0.8904011389042907, 0.8903912754126162, 3.0847563630832324, 3.084755798158117, 1.5799437244858199, 2.332008334705276, 2.3323816089753557, 0.827597640327556, 2.3323816089753557, 0.8276116928398045, 0.8276072440545198, 0.8275929090797135, 0.8276078795952748, 0.8275912849200063, 0.8276191780975852, 0.8276090094455059, 0.8276090094455059, 0.8276068203606832, 0.8276174833222386, 0.827609362523703, 0.827609362523703, 0.827617695169157, 0.8276022303441196, 0.8276120459180017, 0.8276164947032865, 0.8275876835223949, 0.8275980640213926, 0.8275950981645361, 0.8276527911419587, 0.07523812154388854, 0.0752381127169336, 0.0752381127169336, 0.12536578953150568, 0.09227254687995888, 0.07528501915543166, 2.287078996771232, 1.5493476077318575, 1.5493011897782685, 1.5493004052494754, 1.5493481307510528, 1.5493646058557071, 0.8115478339326885, 0.8115657473401299, 0.8115557445980185, 0.8115609747899721, 0.8115611709221704, 0.8115626746023571, 0.8115599287515814, 0.8115665972463224, 0.8115614324317681, 0.8115575751652023, 2.2871668639960543, 0.8115514950670562, 0.8115441074209215, 0.8115659434723282, 0.8115520180862514, 0.8115593403549867, 0.8115614324317681, 0.5437680533128029, 0.8115731349862645, 0.8115550254466248, 0.8115236442949026, 0.8115606479029751, 0.5437572333531986, 0.07377888557734658, 0.07378556224426246, 2.356301088485154, 1.5962033631776884, 1.5962042610932694, 0.836101468976454, 0.8360922332733353, 0.836097556629994, 0.8360977490404756, 0.8360922332733353, 0.8361036496285793, 0.8361013407027996, 0.8360945421991149, 0.8361039061758881, 0.8361039061758881, 0.8360949911569054, 0.8360987110928838, 0.8360940932413244, 0.8360602931333828, 1.596220295300073, 0.8361004427872186, 0.8360681178263029, 0.8361060868280135, 0.8360861402747499, 0.8361052530492596, 0.8361081392064843, 1.596196564674004, 0.8369143391245613, 0.8361064075121495, 0.8360810093285729, 0.8360829975702164, 0.07605728188550515, 0.0761670520652815, 2.2821892351222086, 2.2815434291482957, 1.546137863820882, 1.546135762955645, 2.28235877494685, 0.8098672672320545, 0.8098782442529188, 0.8098744101738609, 0.8098798199018467, 0.8098770887770383, 0.8098744626954918, 0.8098736748710279, 1.5460549846872749, 0.809880292596525, 0.8098552397785715, 0.8098790320773828, 0.8098849145000468, 0.8098620150689615, 0.8098757757362651, 0.0736255903662575, 0.07362558380105363, 0.07362558380105363, 0.07362558380105363, 0.07362558380105363, 0.07362558380105363, 0.0736255903662575, 0.07362558380105363, 0.07362558380105363, 0.07362558380105363, 0.07362558380105363, 0.07363615377927826, 0.0736255903662575, 0.0736255903662575, 0.0736255903662575, 0.0736255903662575, 0.0736255903662575, 0.0736255903662575, 0.0736255903662575, 0.0736255903662575, 0.0736255903662575, 1.5475987421803024, 1.547618374548315, 1.5476218625324796, 2.2845786036298374, 1.5475626664583706, 0.8106413034861117, 0.8106433962766105, 0.8106457880371806, 0.8106347261445441, 0.8106522158937126, 0.8106477313426438, 0.8106447914702765, 0.8106244116770858, 0.8106292450265711, 0.8106523653787483, 0.8106529134905456, 0.8106402570908623, 0.8106564014747102, 0.8106423498813612, 0.8106462863206327, 0.07369540628238026, 0.07369540005383712, 0.07369540005383712, 0.07369540005383712, 0.07369540005383712, 0.07369540005383712, 0.07369540005383712, 0.07369540005383712, 0.07369540005383712, 0.07369540005383712, 0.07374132933103397, 0.07369540628238026, 0.07369540628238026, 0.07369540628238026, 0.07369540628238026, 0.07369540628238026, 0.07369540628238026, 0.07369540628238026, 0.07369540628238026, 0.07369540628238026, 1.5021339543043164, 1.5021334658237062, 1.502133075039218, 1.5020785606031064, 1.502128483321481, 1.046447587455409, 0.7868342601606149, 0.7868456417588353, 0.7868293265064507, 0.7868371910442767, 0.7868308896444036, 0.7868265910150328, 0.7867968425458645, 0.7868103246107092, 2.217473166529393, 0.7868312315808309, 0.7868347974892862, 0.7868289845700234, 0.7868312804288919, 0.07157498168140951, 0.0715304017197103, 0.0715304017197103, 0.0715304017197103, 0.0715304017197103, 0.0715304017197103, 0.07153040782571791, 0.0715304017197103, 0.0715304017197103, 0.0715304017197103, 0.0715304017197103, 0.07157498778741714, 0.07153529263182111, 0.07153040782571791, 0.07153040782571791, 0.07153040782571791, 0.07153040782571791, 0.07153040782571791, 0.07153040782571791, 0.07153040782571791, 0.07153040782571791, 2.209145130352863, 1.4965160546786012, 1.4964691048113377, 1.4965336725928635, 1.4965246762111124, 1.4965430438238543, 0.7838897435174813, 0.7838794820195466, 0.7839008015700504, 0.7838872132851139, 0.7838866978674094, 0.7838824339573086, 1.49653723366064, 0.7838823402449987, 0.7838968656530343, 0.7838610206944949, 0.7838634103583976, 0.07126288179654097, 0.0712628759395216, 0.0712628759395216, 0.0712628759395216, 0.0712628759395216, 0.0712628759395216, 0.0712628759395216, 0.0712628759395216, 0.0712628759395216, 0.07126288179654097, 0.0712628759395216, 0.0712628759395216, 0.0712628759395216, 0.07126288179654097, 0.07126288179654097, 0.07126288179654097, 0.07126288179654097, 0.07126288179654097, 0.07126288179654097, 0.07126288179654097, 0.07126288179654097, 0.07126288179654097, 0.07126288179654097, 0.07126288179654097, 2.078330169380227, 0.7375139602149732, 0.7375147582173543, 0.7375368362832314, 0.7375188546295773, 0.7375237490441814, 0.737512364210211, 0.7375067781935433, 0.7375141198154495, 0.06705771613870634, 0.06704601875380335, 0.06704601210378351, 0.06704601210378351, 0.06704601875380335, 0.06704601210378351, 0.06704601210378351, 0.06704601210378351, 0.06704601210378351, 0.06704601875380335, 0.06704601210378351, 0.06704601210378351, 0.06704601875380335, 0.06704601875380335, 0.06704601875380335, 0.06704601210378351, 0.06704601210378351, 0.06704601875380335, 0.06704601210378351, 0.06704601210378351, 0.06704601210378351, 0.06704601875380335, 0.06704601875380335, 0.06704601875380335, 0.06704601875380335, 0.06704601875380335, 0.06704601875380335, 0.06704601875380335, 0.06704601875380335, 0.06704601875380335, 0.06704601875380335, 0.06704601875380335, 0.06704601875380335, 0.06704601875380335, 0.06704601875380335, 1.2324268506421707, 1.2324160210991482, 1.2324952010366566, 0.6455753345242774, 0.6455858090003155, 0.6455726715218949, 0.6455692540021706, 0.05868892752725094, 0.05868892752725094, 0.05868893307517257, 0.05868892752725094, 0.05868892752725094, 0.05868892752725094, 0.05868892752725094, 0.05868893307517257, 0.05868892752725094, 0.05868892752725094, 0.05868893307517257, 0.05868893307517257, 0.05868893307517257, 0.05868892752725094, 0.05868892752725094, 0.05868893307517257, 0.05868892752725094, 0.05868892752725094, 0.05868892752725094, 0.05868892752725094, 0.05868892752725094, 0.05868893307517257, 0.05868893307517257, 0.058972892347978596, 0.058695951196034955, 0.05868893307517257, 0.05868893307517257, 0.05868893307517257, 0.05868893307517257, 0.05868893307517257, 0.05868893307517257, 0.05868893307517257, 0.05868893307517257, 0.05868893307517257, 0.05868893307517257, 0.05868893307517257, 0.05868893307517257, 0.05868893307517257, 0.05868893307517257, 0.05868893307517257, 0.05868893307517257], \"Total\": [2.0, 5.0, 2.0, 5.0, 2.0, 1.0, 1.0, 3.0, 3.0, 3.0, 5.0, 2.0, 8.0, 5.0, 3.0, 6.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.148895902691975, 2.3407294323533536, 2.3407270963707476, 3.910175437620851, 3.8864410717954634, 3.056027800433573, 3.0776794873541826, 5.317685425041091, 2.1181513796380944, 2.2017171073467754, 2.243892687222647, 2.2465689531856765, 2.246569689998609, 2.2675206723030747, 2.2675205468412725, 2.267521437678914, 2.2682144265893776, 2.2682192327907416, 2.283642939494036, 2.291359967702774, 2.29136138350448, 5.090654117951593, 5.301867078665914, 2.854408020178538, 2.961872263269361, 2.9843514910181135, 3.0037773719987215, 3.0037776462914265, 3.005170549180239, 3.02058892546686, 6.118060169390749, 3.0437354206824967, 5.256917396634538, 3.7313402079498736, 3.731339643024758, 2.2265780481002846, 3.694174766423958, 3.71674160188345, 2.144650231307768, 6.118060169390749, 2.1868193596343337, 2.1868154384646945, 2.186814691774825, 2.189490661446598, 2.189490369825773, 2.211142349054934, 2.211143242295491, 2.2119739178564988, 2.2119744624245103, 2.2342864105242812, 2.2342872047447284, 2.2342872047447284, 2.283642939494036, 2.9047429612829028, 2.947394196097998, 2.972068824478763, 3.02058892546686, 3.0437354206824967, 5.256917396634538, 5.301867078665914, 1.895604508668149, 1.8956081282543957, 2.0687999247355506, 3.910175437620851, 3.148895902691975, 5.075642265965217, 2.9351831025950905, 2.1973907066734064, 2.197391195546995, 2.1973904168752214, 3.7175855632012214, 3.7669305271575424, 2.130058862222281, 2.172225433598558, 2.172225692354382, 2.1958528813411826, 2.196552247323347, 2.2119744624245103, 2.2119739178564988, 2.2196943108023364, 2.219695944491467, 2.2196943961506808, 8.09950232477651, 2.8848563755170504, 2.9323173553193467, 2.9559445858029707, 2.972068824478763, 2.9797958748895743, 2.9843514910181135, 2.166735353839666, 3.646126245690021, 3.71674160188345, 5.090654117951593, 5.256917396634538, 3.910175437620851, 1.895604508668149, 3.6907014106475975, 3.0021131372472247, 2.9797958748895743, 3.7175855632012214, 2.0687999247355506, 2.1972110713599724, 2.218163841492858, 2.2188621290913533, 2.2188610978752816, 2.2196943961506808, 2.219695944491467, 2.2196943108023364, 2.2342872047447284, 2.2342872047447284, 2.2342864105242812, 2.29136138350448, 2.291359967702774, 2.6556786695188217, 5.256917396634538, 2.9047429612829028, 2.907160388277571, 2.9323173553193467, 2.9559445858029707, 2.972068824478763, 3.0437354206824967, 6.118060169390749, 3.6907014106475975, 3.7669305271575424, 5.317685425041091, 8.09950232477651, 1.8956081282543957, 2.93058967701349, 2.93058967701349, 3.6907014106475975, 3.0037776462914265, 3.0037773719987215, 5.301867078665914, 2.1733675116534217, 2.1950205214677148, 2.1958528813411826, 2.218163841492858, 2.267521437678914, 2.2675205468412725, 2.2675206723030747, 5.075642265965217, 2.854408020178538, 2.931974588055571, 2.947394196097998, 2.9559445858029707, 3.8864410717954634, 8.09950232477651, 1.895604508668149, 1.8956081282543957, 2.0687999247355506, 2.1181513796380944, 2.130058862222281, 2.144650231307768, 2.147075108281307, 2.1470751363226546, 2.1524246309030826, 2.152425021687571, 2.152425510168181, 2.9351831025950905, 3.910175437620851, 6.118060169390749, 2.961872263269361, 2.7332064821053996, 2.2682144265893776, 2.2265780481002846, 2.166735353839666, 2.907160388277571, 2.1973904168752214, 2.1957252997100403, 2.931974588055571, 3.005170549180239, 5.075642265965217, 3.646126245690021, 2.1950205214677148, 2.196552247323347, 2.211143242295491, 2.211142349054934, 2.2188610978752816, 2.2188621290913533, 2.2682192327907416, 2.2682144265893776, 2.84184810938758, 2.84452310622296, 2.889378113465062, 2.947394196097998, 3.02058892546686, 3.0776794873541826, 5.317685425041091, 1.895604508668149, 1.8956081282543957, 2.0687999247355506, 2.1181513796380944, 2.130058862222281, 2.144650231307768, 2.147075108281307, 2.1470751363226546, 2.1524246309030826, 2.152425021687571, 2.7332064821053996, 5.090654117951593, 3.910175437620851, 6.118060169390749, 2.3407270963707476, 2.961872263269361, 2.2265780481002846, 8.09950232477651, 2.166735353839666, 2.93058967701349, 2.152425510168181, 2.152425021687571, 2.1524246309030826, 2.889378113465062, 2.961872263269361, 2.166735353839666, 2.1733675116534217, 2.189490369825773, 2.189490661446598, 2.1972110713599724, 2.246569689998609, 2.2465689531856765, 2.84452310622296, 2.862372677127916, 8.09950232477651, 2.9843514910181135, 3.056027800433573, 3.694174766423958, 5.256917396634538, 1.895604508668149, 1.8956081282543957, 2.0687999247355506, 2.1181513796380944, 2.130058862222281, 2.144650231307768, 2.147075108281307, 2.1470751363226546, 2.172225433598558, 2.172225692354382, 2.186814691774825, 3.646126245690021, 3.910175437620851, 5.090654117951593, 6.118060169390749, 2.3407270963707476, 2.7332064821053996, 5.075642265965217, 3.02058892546686, 2.2682144265893776, 2.2265780481002846, 2.8597042119969167, 2.1470751363226546, 2.147075108281307, 2.862372677127916, 2.8848563755170504, 2.907160388277571, 2.172225692354382, 2.172225433598558, 2.186814691774825, 2.1868154384646945, 2.1868193596343337, 2.243892687222647, 5.090654117951593, 2.84184810938758, 2.9323173553193467, 3.646126245690021, 8.09950232477651, 1.895604508668149, 1.8956081282543957, 2.0687999247355506, 2.1181513796380944, 2.130058862222281, 2.144650231307768, 2.1524246309030826, 2.152425021687571, 2.152425510168181, 2.166735353839666, 2.1733675116534217, 2.189490369825773, 2.189490661446598, 3.910175437620851, 6.118060169390749, 2.3407270963707476, 2.961872263269361, 2.7332064821053996, 5.075642265965217, 2.2682144265893776, 2.1973904168752214, 2.6556786695188217, 2.889378113465062, 3.694174766423958, 2.7332064821053996, 2.130058862222281, 2.144650231307768, 2.2017171073467754, 2.84184810938758, 2.84452310622296, 2.9047429612829028, 5.075642265965217, 5.317685425041091, 1.8956081282543957, 1.895604508668149, 2.0687999247355506, 2.1181513796380944, 2.147075108281307, 2.1470751363226546, 2.1524246309030826, 2.152425021687571, 2.152425510168181, 2.166735353839666, 2.172225433598558, 2.172225692354382, 2.1733675116534217, 2.186814691774825, 2.1868154384646945, 2.1868193596343337, 2.189490369825773, 2.189490661446598, 2.1950205214677148, 2.1957252997100403, 2.1958528813411826, 5.090654117951593, 3.910175437620851, 6.118060169390749, 2.3407270963707476, 2.961872263269361, 3.02058892546686, 2.2682144265893776, 2.2265780481002846, 8.09950232477651, 2.93058967701349, 3.7669305271575424, 2.907160388277571, 2.1973904168752214, 2.197391195546995, 1.895604508668149, 1.8956081282543957, 2.6556786695188217, 2.0687999247355506, 2.1181513796380944, 2.854408020178538, 5.090654117951593, 2.130058862222281, 2.144650231307768, 2.147075108281307, 2.1470751363226546, 2.1524246309030826, 2.152425021687571, 2.152425510168181, 2.166735353839666, 2.172225433598558, 2.172225692354382, 2.1733675116534217, 2.186814691774825, 2.1868154384646945, 2.1868193596343337, 2.189490369825773, 2.189490661446598, 2.1950205214677148, 2.1957252997100403, 2.1958528813411826, 2.196552247323347, 2.1972110713599724, 2.1973904168752214, 2.1973907066734064, 3.694174766423958, 2.7332064821053996, 3.910175437620851, 6.118060169390749, 5.317685425041091, 2.3407270963707476, 2.84452310622296, 2.961872263269361, 5.075642265965217, 3.02058892546686, 2.2682144265893776, 2.2265780481002846, 8.09950232477651, 5.301867078665914, 2.93058967701349, 3.7669305271575424, 2.907160388277571, 2.291359967702774], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.9798, -3.3619, -3.3619, -2.8812, -2.9725, -3.3619, -3.3619, -2.9725, -4.0086, -4.0086, -4.0086, -4.0086, -4.0086, -4.0086, -4.0086, -4.0086, -4.0085, -4.0086, -4.0086, -4.0086, -4.0086, -3.3619, -3.3619, -4.0086, -4.0086, -4.0086, -4.0086, -4.0086, -4.0086, -4.0085, -3.3619, -4.0085, -4.0086, -2.5088, -2.5088, -3.1779, -2.7885, -2.7884, -3.8245, -2.7884, -3.8245, -3.8245, -3.8245, -3.8245, -3.8245, -3.8245, -3.8245, -3.8245, -3.8245, -3.8245, -3.8245, -3.8245, -3.8245, -3.8245, -3.8245, -3.8245, -3.8245, -3.8245, -3.8245, -3.8244, -6.2224, -6.2224, -6.2224, -5.7118, -6.0183, -6.2217, -2.7309, -3.1203, -3.1204, -3.1204, -3.1203, -3.1203, -3.767, -3.767, -3.767, -3.767, -3.767, -3.767, -3.767, -3.767, -3.767, -3.767, -2.7309, -3.767, -3.767, -3.767, -3.767, -3.767, -3.767, -4.1674, -3.767, -3.767, -3.767, -3.767, -4.1674, -6.1649, -6.1648, -2.6819, -3.0714, -3.0714, -3.718, -3.718, -3.718, -3.718, -3.718, -3.718, -3.718, -3.718, -3.718, -3.718, -3.718, -3.718, -3.718, -3.7181, -3.0714, -3.718, -3.7181, -3.718, -3.718, -3.718, -3.718, -3.0714, -3.7171, -3.718, -3.7181, -3.718, -6.1153, -6.1138, -2.5141, -2.5144, -2.9035, -2.9035, -2.514, -3.5501, -3.5501, -3.5501, -3.5501, -3.5501, -3.5501, -3.5501, -2.9035, -3.5501, -3.5501, -3.5501, -3.5501, -3.5501, -3.5501, -5.948, -5.948, -5.948, -5.948, -5.948, -5.948, -5.948, -5.948, -5.948, -5.948, -5.948, -5.9478, -5.948, -5.948, -5.948, -5.948, -5.948, -5.948, -5.948, -5.948, -5.948, -2.8499, -2.8499, -2.8499, -2.4604, -2.8499, -3.4965, -3.4965, -3.4965, -3.4965, -3.4965, -3.4965, -3.4965, -3.4965, -3.4965, -3.4965, -3.4965, -3.4965, -3.4965, -3.4965, -3.4965, -5.8944, -5.8944, -5.8944, -5.8944, -5.8944, -5.8944, -5.8944, -5.8944, -5.8944, -5.8944, -5.8938, -5.8944, -5.8944, -5.8944, -5.8944, -5.8944, -5.8944, -5.8944, -5.8944, -5.8944, -2.8598, -2.8598, -2.8598, -2.8599, -2.8598, -3.2213, -3.5065, -3.5064, -3.5065, -3.5065, -3.5065, -3.5065, -3.5065, -3.5065, -2.4703, -3.5065, -3.5065, -3.5065, -3.5065, -5.9037, -5.9043, -5.9043, -5.9043, -5.9043, -5.9043, -5.9043, -5.9043, -5.9043, -5.9043, -5.9043, -5.9037, -5.9043, -5.9043, -5.9043, -5.9043, -5.9043, -5.9043, -5.9043, -5.9043, -5.9043, -2.4325, -2.8219, -2.822, -2.8219, -2.8219, -2.8219, -3.4686, -3.4686, -3.4686, -3.4686, -3.4686, -3.4686, -2.8219, -3.4686, -3.4686, -3.4686, -3.4686, -5.8665, -5.8665, -5.8665, -5.8665, -5.8665, -5.8665, -5.8665, -5.8665, -5.8665, -5.8665, -5.8665, -5.8665, -5.8665, -5.8665, -5.8665, -5.8665, -5.8665, -5.8665, -5.8665, -5.8665, -5.8665, -5.8665, -5.8665, -5.8665, -1.9274, -2.9634, -2.9634, -2.9634, -2.9634, -2.9634, -2.9634, -2.9634, -2.9634, -5.3611, -5.3613, -5.3613, -5.3613, -5.3613, -5.3613, -5.3613, -5.3613, -5.3613, -5.3613, -5.3613, -5.3613, -5.3613, -5.3613, -5.3613, -5.3613, -5.3613, -5.3613, -5.3613, -5.3613, -5.3613, -5.3613, -5.3613, -5.3613, -5.3613, -5.3613, -5.3613, -5.3613, -5.3613, -5.3613, -5.3613, -5.3613, -5.3613, -5.3613, -5.3613, -2.2687, -2.2687, -2.2687, -2.9153, -2.9153, -2.9153, -2.9153, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3084, -5.3131, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132, -5.3132], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.5396, 1.454, 1.454, 1.4217, 1.3365, 1.1874, 1.1803, 1.0229, 0.9073, 0.8686, 0.8497, 0.8485, 0.8485, 0.8392, 0.8392, 0.8392, 0.8389, 0.8389, 0.8321, 0.8287, 0.8287, 0.6771, 0.6364, 0.609, 0.5721, 0.5645, 0.558, 0.558, 0.5575, 0.5524, 0.4933, 0.5448, -0.0017, 1.8409, 1.8409, 1.6881, 1.5711, 1.5652, 1.079, 1.0668, 1.0595, 1.0595, 1.0595, 1.0583, 1.0583, 1.0485, 1.0484, 1.0481, 1.0481, 1.038, 1.038, 1.038, 1.0162, 0.7756, 0.761, 0.7527, 0.7365, 0.7289, 0.1824, 0.174, -1.1955, -1.1955, -1.2829, -1.4089, -1.4989, -2.1798, 1.8588, 1.7588, 1.7588, 1.7588, 1.233, 1.2198, 1.1433, 1.1237, 1.1237, 1.1129, 1.1126, 1.1056, 1.1056, 1.1021, 1.1021, 1.1021, 0.8438, 0.84, 0.8236, 0.8156, 0.8102, 0.8076, 0.8061, 0.7258, 0.6058, 0.5866, 0.272, 0.2399, 0.1354, -1.138, -1.8042, 1.8852, 1.5032, 1.282, 1.2214, 1.1612, 1.1517, 1.1514, 1.1514, 1.151, 1.151, 1.151, 1.1445, 1.1445, 1.1445, 1.1193, 1.1192, 0.9717, 0.9355, 0.8821, 0.8812, 0.8726, 0.8646, 0.8591, 0.8353, 0.7838, 0.6436, 0.6221, 0.2773, -0.1434, -1.0884, -1.5226, 2.0771, 1.8462, 1.6631, 1.6631, 1.4843, 1.34, 1.3301, 1.3298, 1.3197, 1.2976, 1.2976, 1.2976, 1.1385, 1.0675, 1.0406, 1.0354, 1.0325, 0.7588, 0.0245, -0.9211, -0.9211, -1.0085, -1.0321, -1.0377, -1.0445, -1.0457, -1.0457, -1.0482, -1.0482, -1.0482, -1.3582, -1.6451, -2.0928, -1.3674, -1.287, -1.1006, -1.082, -1.0548, -1.3487, -1.0688, 2.03, 1.7409, 1.7162, 1.5816, 1.5229, 1.3837, 1.383, 1.3764, 1.3764, 1.3729, 1.3729, 1.3509, 1.3509, 1.1254, 1.1245, 1.1089, 1.089, 1.0645, 1.0457, 0.4989, -0.8675, -0.8675, -0.9549, -0.9785, -0.9841, -0.991, -0.9921, -0.9921, -0.9946, -0.9946, -1.2328, -1.8554, -1.5916, -2.0392, -1.0784, -1.3138, -1.0284, -2.3198, -1.0012, -1.3032, 2.04, 2.04, 2.04, 1.7455, 1.7208, 1.6719, 1.3837, 1.3763, 1.3763, 1.3728, 1.3506, 1.3506, 1.1145, 1.1083, 1.1043, 1.0666, 1.0429, 0.8532, 0.5004, -0.8768, -0.8775, -0.9649, -0.9885, -0.9941, -1.0009, -1.002, -1.002, -1.0137, -1.0137, -1.0204, -1.531, -1.6014, -1.8653, -2.0492, -1.0884, -1.2434, -1.8624, -1.3434, -1.0569, -1.0384, 2.1832, 2.0804, 2.0803, 1.7928, 1.785, 1.7773, 1.4221, 1.4221, 1.4154, 1.4154, 1.4154, 1.3896, 1.2171, 1.1534, 1.1221, 0.9042, 0.106, -0.8396, -0.8396, -0.927, -0.9506, -0.9562, -0.963, -0.9666, -0.9666, -0.9666, -0.9733, -0.9763, -0.9837, -0.9837, -1.5636, -2.0113, -1.0505, -1.2859, -1.2055, -1.8245, -1.019, -0.9873, -1.1767, -1.2611, -1.5068, 2.7336, 1.9469, 1.9401, 1.9138, 1.6586, 1.6577, 1.6367, 1.0786, 1.032, -0.3342, -0.3344, -0.4218, -0.4454, -0.459, -0.459, -0.4615, -0.4615, -0.4615, -0.4681, -0.4706, -0.4706, -0.4711, -0.4773, -0.4773, -0.4773, -0.4785, -0.4785, -0.4811, -0.4814, -0.4814, -1.3223, -1.0584, -1.5061, -0.5453, -0.7807, -0.8003, -0.5139, -0.4953, -1.7867, -0.7701, -1.0211, -0.762, -0.4821, -0.4821, 2.7582, 2.7581, 2.421, 2.0241, 2.0006, 1.7022, 1.1237, -0.4029, -0.4098, -0.4109, -0.4109, -0.4134, -0.4134, -0.4134, -0.42, -0.4225, -0.4225, -0.4231, -0.4292, -0.4292, -0.4292, -0.4305, -0.4305, -0.433, -0.4333, -0.4334, -0.4337, -0.434, -0.4341, -0.4341, -0.9487, -0.6522, -1.0104, -1.458, -1.3178, -0.4973, -0.6922, -0.7326, -1.2713, -0.7522, -0.4658, -0.4473, -1.7386, -1.3149, -0.722, -0.9731, -0.714, -0.4759]}, \"token.table\": {\"Topic\": [3, 4, 8, 3, 6, 1, 2, 4, 3, 4, 1, 5, 1, 3, 4, 5, 7, 8, 2, 3, 7, 1, 2, 4, 1, 5, 2, 6, 8, 3, 4, 1, 4, 4, 5, 4, 6, 1, 1, 3, 4, 3, 10, 1, 5, 1, 7, 3, 8, 6, 7, 9, 1, 2, 5, 6, 9, 2, 9, 2, 5, 6, 7, 8, 1, 6, 9, 2, 6, 8, 2, 3, 2, 4, 4, 8, 1, 2, 6, 1, 1, 8, 7, 3, 4, 5, 5, 6, 3, 8, 1, 9, 4, 10, 1, 6, 2, 3, 4, 1, 5, 10, 2, 4, 2, 8, 2, 8, 1, 5, 3, 8, 1, 10, 1, 4, 6, 9, 6, 3, 4, 1, 6, 3, 5, 2, 7, 4, 6, 1, 4, 6, 8, 9, 1, 6, 5, 6, 4, 7, 1, 7, 1, 3, 8, 10, 2, 4, 7, 1, 3, 7, 4, 5, 1, 7, 2, 4, 9, 2, 8, 5, 7, 3, 5, 3, 4, 8, 2, 3, 6, 7, 1, 5, 3, 4, 10, 1, 2, 5, 10, 1, 5, 3, 3, 6, 8, 4, 2, 7, 1, 1, 2, 3, 4, 7, 7, 2, 3, 9, 2, 7, 2, 3, 1, 7, 1, 3, 3, 4, 2], \"Freq\": [0.34102720777679746, 0.34102720777679746, 0.34102720777679746, 0.4552589182518058, 0.4552589182518058, 0.32690100205391814, 0.32690100205391814, 0.32690100205391814, 0.33559345739984897, 0.6711869147996979, 0.4410101635129953, 0.4410101635129953, 0.12346437594578911, 0.24692875189157823, 0.12346437594578911, 0.12346437594578911, 0.24692875189157823, 0.12346437594578911, 0.804000662713171, 0.4615238304151463, 0.4615238304151463, 0.32854366815357755, 0.32854366815357755, 0.32854366815357755, 0.4410103367720445, 0.4410103367720445, 0.45225491720486055, 0.45225491720486055, 0.46574989209412476, 0.450512485045086, 0.450512485045086, 0.43642204371867427, 0.43642204371867427, 0.4508233257138412, 0.4508233257138412, 0.4506814492388088, 0.4506814492388088, 0.8544345076180861, 0.26546812923427654, 0.5309362584685531, 0.26546812923427654, 0.6813884960811253, 0.5275362004190418, 0.3329141227329648, 0.6658282454659296, 0.44512321715386544, 0.44512321715386544, 0.4603573208436473, 0.4603573208436473, 0.35155277797262435, 0.35155277797262435, 0.35155277797262435, 0.4378968282237503, 0.4378968282237503, 0.39403880242132605, 0.39403880242132605, 0.19701940121066303, 0.46627649833148715, 0.46627649833148715, 0.33928274722257445, 0.33928274722257445, 0.33928274722257445, 0.34936051758410186, 0.34936051758410186, 0.33275981633481116, 0.6655196326696223, 0.7317412764437, 0.45225473450641457, 0.45225473450641457, 0.4657498860113127, 0.5381057426716199, 0.26905287133580996, 0.4475700339134565, 0.4475700339134565, 0.3439782696655681, 0.3439782696655681, 0.3310612680755428, 0.3310612680755428, 0.3310612680755428, 0.6351432571302882, 0.4456541106864334, 0.4456541106864334, 0.9291845150693961, 0.33830133514778116, 0.33830133514778116, 0.33830133514778116, 0.3410670761178666, 0.6821341522357331, 0.46035737568148133, 0.46035737568148133, 0.4541909569867814, 0.4541909569867814, 0.48337202067900664, 0.48337202067900664, 0.44087542530256263, 0.44087542530256263, 0.33646596329254874, 0.33646596329254874, 0.33646596329254874, 0.3503353385117843, 0.3503353385117843, 0.3503353385117843, 0.4475701930109074, 0.4475701930109074, 0.45728596131645827, 0.45728596131645827, 0.4572861174571665, 0.4572861174571665, 0.4410103123709652, 0.4410103123709652, 0.3466377073350041, 0.3466377073350041, 0.47210978856990815, 0.47210978856990815, 0.5641552217197613, 0.18805174057325377, 0.18805174057325377, 0.18805174057325377, 0.9108607530569115, 0.4505121534693349, 0.4505121534693349, 0.6498402475689107, 0.32492012378445534, 0.9101705298433409, 0.6824565089023868, 0.4567272885879714, 0.4567272885879714, 0.4506816586930888, 0.4506816586930888, 0.4364217740593012, 0.4364217740593012, 0.3518836902988107, 0.3518836902988107, 0.3518836902988107, 0.44087449111770083, 0.44087449111770083, 0.4555766063322923, 0.4555766063322923, 0.45512241087564065, 0.45512241087564065, 0.44512307116571986, 0.44512307116571986, 0.3928768196894845, 0.19643840984474226, 0.19643840984474226, 0.19643840984474226, 0.4475700339134565, 0.4475700339134565, 0.9291846837679373, 0.33508117358483447, 0.33508117358483447, 0.33508117358483447, 0.27095120648747706, 0.5419024129749541, 0.6544443082998952, 0.3272221541499476, 0.34426454021196495, 0.34426454021196495, 0.34426454021196495, 0.4572851413603791, 0.4572851413603791, 0.4601154635090846, 0.4601154635090846, 0.4554039154887372, 0.4554039154887372, 0.45051246772265874, 0.45051246772265874, 0.6993730301230736, 0.4520847853297166, 0.4520847853297166, 0.3460952359747609, 0.6921904719495218, 0.7719144442383262, 0.2573048147461087, 0.91017020731356, 0.3765515803842294, 0.3765515803842294, 0.3772256019106483, 0.18861280095532415, 0.3772256019106483, 0.5275351931102278, 0.33291415313332534, 0.6658283062666507, 0.9101704098074425, 0.2742636794823193, 0.5485273589646386, 0.2742636794823193, 0.6661974111455012, 0.5413929027337393, 0.27069645136686965, 0.8544353603207147, 0.19022554941422454, 0.19022554941422454, 0.19022554941422454, 0.3804510988284491, 0.19022554941422454, 0.9291843041963058, 0.8040007844389346, 0.46947059432747523, 0.46947059432747523, 0.456727227755929, 0.456727227755929, 0.4520848966289099, 0.4520848966289099, 0.3376242832620284, 0.6752485665240568, 0.7672290023450591, 0.2557430007816863, 0.5379835826233937, 0.5379835826233937, 0.8982393416239772], \"Term\": [\"10\", \"10\", \"10\", \"account\", \"account\", \"actually\", \"actually\", \"actually\", \"ago\", \"ago\", \"alcohol\", \"alcohol\", \"amp\", \"amp\", \"amp\", \"amp\", \"amp\", \"amp\", \"bbc\", \"billym2k\", \"billym2k\", \"buy\", \"buy\", \"buy\", \"california\", \"california\", \"case\", \"case\", \"cbdoge\", \"check\", \"check\", \"company\", \"company\", \"crime\", \"crime\", \"criminal\", \"criminal\", \"davidasinclair\", \"davidmweissman\", \"davidmweissman\", \"davidmweissman\", \"day\", \"dogeofficialceo\", \"drug\", \"drug\", \"end\", \"end\", \"fix\", \"fix\", \"friend\", \"friend\", \"friend\", \"fund\", \"fund\", \"good\", \"good\", \"good\", \"great\", \"great\", \"greatly\", \"greatly\", \"greatly\", \"heavy\", \"heavy\", \"help\", \"help\", \"iamharaldur\", \"importantly\", \"importantly\", \"industry\", \"influence\", \"influence\", \"investmattallen\", \"investmattallen\", \"jonerlichman\", \"jonerlichman\", \"kanekoathegreat\", \"kanekoathegreat\", \"kanekoathegreat\", \"know\", \"lack\", \"lack\", \"launch\", \"legacy\", \"legacy\", \"legacy\", \"let\", \"let\", \"live\", \"live\", \"lol\", \"lol\", \"long\", \"long\", \"matter\", \"matter\", \"medium\", \"medium\", \"medium\", \"micsolana\", \"micsolana\", \"micsolana\", \"need\", \"need\", \"oh\", \"oh\", \"old\", \"old\", \"open\", \"open\", \"paulg\", \"paulg\", \"pay\", \"pay\", \"people\", \"people\", \"people\", \"people\", \"platform\", \"post\", \"post\", \"press\", \"press\", \"principle\", \"probably\", \"problem\", \"problem\", \"propaganda\", \"propaganda\", \"public\", \"public\", \"question\", \"question\", \"question\", \"rapidly\", \"rapidly\", \"reduce\", \"reduce\", \"remove\", \"remove\", \"require\", \"require\", \"right\", \"right\", \"right\", \"right\", \"robbysoave\", \"robbysoave\", \"rt\", \"say\", \"say\", \"say\", \"scottadamssays\", \"scottadamssays\", \"sf\", \"sf\", \"shellenbergermd\", \"shellenbergermd\", \"shellenbergermd\", \"silly\", \"silly\", \"starship\", \"starship\", \"supplement\", \"supplement\", \"take\", \"take\", \"talent\", \"tbh\", \"tbh\", \"tell\", \"tell\", \"tesla\", \"tesla\", \"teslaownerssv\", \"therabbithole84\", \"therabbithole84\", \"think\", \"think\", \"think\", \"thinking\", \"time\", \"time\", \"timsweeneyepic\", \"titterdaily\", \"titterdaily\", \"titterdaily\", \"true\", \"trungtphan\", \"trungtphan\", \"try\", \"twitter\", \"twitter\", \"twitter\", \"twitter\", \"twitter\", \"w\", \"wallstreetsilv\", \"way\", \"way\", \"world\", \"world\", \"wow\", \"wow\", \"wrong\", \"wrong\", \"yeah\", \"yeah\", \"year\", \"year\", \"yes\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [6, 2, 5, 9, 4, 7, 10, 3, 1, 8]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el6841400969619949288016657665\", ldavis_el6841400969619949288016657665_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el6841400969619949288016657665\", ldavis_el6841400969619949288016657665_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el6841400969619949288016657665\", ldavis_el6841400969619949288016657665_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dominant Topic per Tweet"
      ],
      "metadata": {
        "id": "xCamzUmICpcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_topic_sents_keywords = format_topics_sentences(ldamodel=final_model, corpus=corpus, tweets=tweets_df['original_tweet'])\n",
        "\n",
        "# Format\n",
        "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
        "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Tweet']\n",
        "\n",
        "# Show\n",
        "df_dominant_topic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1VrZeGhMCu0e",
        "outputId": "4c105522-b68d-45f9-ce7e-99404a91c211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
            "<ipython-input-51-2bc4fb0735af>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
              "0             0               7              0.7000   \n",
              "1             1               0              0.1000   \n",
              "2             2               8              0.7750   \n",
              "3             3               5              0.5500   \n",
              "4             4               5              0.9400   \n",
              "..          ...             ...                 ...   \n",
              "94           94               5              0.3667   \n",
              "95           95               2              0.8500   \n",
              "96           96               1              0.5500   \n",
              "97           97               2              0.5500   \n",
              "98           98               0              0.1000   \n",
              "\n",
              "                                             Keywords  \\\n",
              "0   therabbithole84, dogeofficialceo, thinking, pa...   \n",
              "1   iamharaldur, lol, friend, question, great, peo...   \n",
              "2   true, twitter, year, ago, actually, scottadams...   \n",
              "3   yeah, tesla, people, know, right, press, actua...   \n",
              "4   yeah, tesla, people, know, right, press, actua...   \n",
              "..                                                ...   \n",
              "94  yeah, tesla, people, know, right, press, actua...   \n",
              "95  talent, jonerlichman, right, heavy, paulg, ind...   \n",
              "96  bbc, wallstreetsilv, actually, influence, trun...   \n",
              "97  talent, jonerlichman, right, heavy, paulg, ind...   \n",
              "98  iamharaldur, lol, friend, question, great, peo...   \n",
              "\n",
              "                                                Tweet  \n",
              "0   @micsolana So inspiring to see that they have ...  \n",
              "1                                      @ChrisJBakke 🤣  \n",
              "2   Final date for removing legacy Blue checks is ...  \n",
              "3                                @MuskUniversity Yeah  \n",
              "4   @Liv_Boeree @micsolana It’s super easy to get ...  \n",
              "..                                                ...  \n",
              "94                         @BillyM2k Yeah, it’s weird  \n",
              "95  @paulg Major misallocation of capital imo. Mos...  \n",
              "96                     @PeterDiamandis After AGI, yes  \n",
              "97                                    @JonErlichman 🎯  \n",
              "98       @FoxNews And they still haven’t invited me 😭  \n",
              "\n",
              "[99 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-133e0209-df73-4d39-a4a1-f1e01feb3ad0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Document_No</th>\n",
              "      <th>Dominant_Topic</th>\n",
              "      <th>Topic_Perc_Contrib</th>\n",
              "      <th>Keywords</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0.7000</td>\n",
              "      <td>therabbithole84, dogeofficialceo, thinking, pa...</td>\n",
              "      <td>@micsolana So inspiring to see that they have ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.1000</td>\n",
              "      <td>iamharaldur, lol, friend, question, great, peo...</td>\n",
              "      <td>@ChrisJBakke 🤣</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>0.7750</td>\n",
              "      <td>true, twitter, year, ago, actually, scottadams...</td>\n",
              "      <td>Final date for removing legacy Blue checks is ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>0.5500</td>\n",
              "      <td>yeah, tesla, people, know, right, press, actua...</td>\n",
              "      <td>@MuskUniversity Yeah</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>0.9400</td>\n",
              "      <td>yeah, tesla, people, know, right, press, actua...</td>\n",
              "      <td>@Liv_Boeree @micsolana It’s super easy to get ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>94</td>\n",
              "      <td>5</td>\n",
              "      <td>0.3667</td>\n",
              "      <td>yeah, tesla, people, know, right, press, actua...</td>\n",
              "      <td>@BillyM2k Yeah, it’s weird</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>95</td>\n",
              "      <td>2</td>\n",
              "      <td>0.8500</td>\n",
              "      <td>talent, jonerlichman, right, heavy, paulg, ind...</td>\n",
              "      <td>@paulg Major misallocation of capital imo. Mos...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>96</td>\n",
              "      <td>1</td>\n",
              "      <td>0.5500</td>\n",
              "      <td>bbc, wallstreetsilv, actually, influence, trun...</td>\n",
              "      <td>@PeterDiamandis After AGI, yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>97</td>\n",
              "      <td>2</td>\n",
              "      <td>0.5500</td>\n",
              "      <td>talent, jonerlichman, right, heavy, paulg, ind...</td>\n",
              "      <td>@JonErlichman 🎯</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>98</td>\n",
              "      <td>0</td>\n",
              "      <td>0.1000</td>\n",
              "      <td>iamharaldur, lol, friend, question, great, peo...</td>\n",
              "      <td>@FoxNews And they still haven’t invited me 😭</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>99 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-133e0209-df73-4d39-a4a1-f1e01feb3ad0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-133e0209-df73-4d39-a4a1-f1e01feb3ad0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-133e0209-df73-4d39-a4a1-f1e01feb3ad0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Most Relevant Tweet per Topic"
      ],
      "metadata": {
        "id": "1iX83AeQEkd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group top 5 sentences under each topic\n",
        "most_relevant_tweet_df = pd.DataFrame()\n",
        "\n",
        "most_relevant_tweet_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
        "\n",
        "for i, grp in most_relevant_tweet_grpd:\n",
        "    most_relevant_tweet_df = pd.concat([most_relevant_tweet_df, \n",
        "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
        "                                            axis=0)\n",
        "\n",
        "# Reset Index    \n",
        "most_relevant_tweet_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Format\n",
        "most_relevant_tweet_df.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Tweet\"]\n",
        "\n",
        "# Show\n",
        "most_relevant_tweet_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "QmeCv_AYDuO6",
        "outputId": "1d9f072d-23de-4546-83f5-7007a8fa440b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Topic_Num  Topic_Perc_Contrib  \\\n",
              "0          0              0.8500   \n",
              "1          1              0.9357   \n",
              "2          2              0.8875   \n",
              "3          3              0.9250   \n",
              "4          4              0.9308   \n",
              "5          5              0.9400   \n",
              "6          6              0.9000   \n",
              "7          7              0.7000   \n",
              "8          8              0.9000   \n",
              "9          9              0.9000   \n",
              "\n",
              "                                            Keywords  \\\n",
              "0  iamharaldur, lol, friend, question, great, peo...   \n",
              "1  bbc, wallstreetsilv, actually, influence, trun...   \n",
              "2  talent, jonerlichman, right, heavy, paulg, ind...   \n",
              "3  think, probably, scottadamssays, drug, time, g...   \n",
              "4  amp, day, davidmweissman, year, timsweeneyepic...   \n",
              "5  yeah, tesla, people, know, right, press, actua...   \n",
              "6  good, help, let, platform, titterdaily, kaneko...   \n",
              "7  therabbithole84, dogeofficialceo, thinking, pa...   \n",
              "8  true, twitter, year, ago, actually, scottadams...   \n",
              "9  amp, w, launch, rt, wrong, tell, billym2k, pro...   \n",
              "\n",
              "                                               Tweet  \n",
              "0         @iamharaldur Great way to make friends lol  \n",
              "1  @EndWokeness We need to add more granularity t...  \n",
              "2  @paulg Once prominent, they’re one of the few ...  \n",
              "3  @micsolana I think we should legalize it. The ...  \n",
              "4  @davidmweissman @TitterDaily Too much corrupti...  \n",
              "5  @Liv_Boeree @micsolana It’s super easy to get ...  \n",
              "6  @AnonOpsUnited I’m told Putin called me a war ...  \n",
              "7  @micsolana So inspiring to see that they have ...  \n",
              "8  @investmattallen @ShellenbergerMD @robbysoave ...  \n",
              "9  Our landlord at SF HQ says we’re legally requi...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ceb62941-3d1f-4bff-8cde-15ee7a0d9132\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Topic_Num</th>\n",
              "      <th>Topic_Perc_Contrib</th>\n",
              "      <th>Keywords</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.8500</td>\n",
              "      <td>iamharaldur, lol, friend, question, great, peo...</td>\n",
              "      <td>@iamharaldur Great way to make friends lol</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.9357</td>\n",
              "      <td>bbc, wallstreetsilv, actually, influence, trun...</td>\n",
              "      <td>@EndWokeness We need to add more granularity t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.8875</td>\n",
              "      <td>talent, jonerlichman, right, heavy, paulg, ind...</td>\n",
              "      <td>@paulg Once prominent, they’re one of the few ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.9250</td>\n",
              "      <td>think, probably, scottadamssays, drug, time, g...</td>\n",
              "      <td>@micsolana I think we should legalize it. The ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.9308</td>\n",
              "      <td>amp, day, davidmweissman, year, timsweeneyepic...</td>\n",
              "      <td>@davidmweissman @TitterDaily Too much corrupti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>0.9400</td>\n",
              "      <td>yeah, tesla, people, know, right, press, actua...</td>\n",
              "      <td>@Liv_Boeree @micsolana It’s super easy to get ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>0.9000</td>\n",
              "      <td>good, help, let, platform, titterdaily, kaneko...</td>\n",
              "      <td>@AnonOpsUnited I’m told Putin called me a war ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>0.7000</td>\n",
              "      <td>therabbithole84, dogeofficialceo, thinking, pa...</td>\n",
              "      <td>@micsolana So inspiring to see that they have ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>0.9000</td>\n",
              "      <td>true, twitter, year, ago, actually, scottadams...</td>\n",
              "      <td>@investmattallen @ShellenbergerMD @robbysoave ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>0.9000</td>\n",
              "      <td>amp, w, launch, rt, wrong, tell, billym2k, pro...</td>\n",
              "      <td>Our landlord at SF HQ says we’re legally requi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ceb62941-3d1f-4bff-8cde-15ee7a0d9132')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ceb62941-3d1f-4bff-8cde-15ee7a0d9132 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ceb62941-3d1f-4bff-8cde-15ee7a0d9132');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    }
  ]
}